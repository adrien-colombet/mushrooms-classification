{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Wild Food UK dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset will be used as a separate validation dataset. It contains images of species not referenced in the main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that to use the Kaggle API, you need to have a Kaggle account and a Kaggle API token. \n",
    "The token is a JSON file that you can download from your Kaggle account settings page. \n",
    "Once downloaded, place it in the location ~/.kaggle/kaggle.json on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import kaggle\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '..'\n",
    "dataset_path = os.path.join(project_path, 'dataset', 'wildfooduk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with your Kaggle account\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "kaggle.api.dataset_download_files('daniilonishchenko/mushrooms-images-classification-215', path=dataset_path, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying scientific name of complementary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only common names are indicated. \n",
    "# creating a dataframe common english name\n",
    "\n",
    "# List of common names\n",
    "reference_table = pd.read_csv(os.path.join(dataset_path, 'mushrooms.txt'), names=['common_name']  )\n",
    "reference_table['scientific_name'] = ''\n",
    "reference_table['edibility'] = ''\n",
    "reference_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get('https://www.wildfooduk.com/mushroom-guide/')\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the <td> elements with class=\"spotlight-text\"\n",
    "td_elements = soup.find_all('td', class_='spotlight-text')\n",
    "\n",
    "# Extract the text from each <td> element\n",
    "scientific_names  = [td.text.strip() for td in td_elements]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the <td> elements with class=\"mushroom-image\"\n",
    "td_elements = soup.find_all('td', class_='mushroom-image')\n",
    "\n",
    "# Find all the <img> elements within the <td> element\n",
    "img_elements = [td.find('img') for td in td_elements] \n",
    "\n",
    "# Extract the text from each <td> element\n",
    "common_names  = [img.get('alt') for img in img_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the <td> elements with class=\"mushroom-icon\"\n",
    "td_elements = soup.find_all('td', class_='mushroom-icon')\n",
    "\n",
    "# Find all the <img> elements within the <td> element\n",
    "img_elements = [td.find('img') for td in td_elements] \n",
    "\n",
    "# Extract the text from each <td> element\n",
    "edibility  = [img.get('alt') for img in img_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table = pd.DataFrame({'common_name': common_names, 'scientific_name': scientific_names, 'edibility' : edibility})\n",
    "reference_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table['edibility'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table['edibility'] = reference_table['edibility'].replace({'Edible': 1, 'Poisonous': 0, 'Inedible': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table['edibility'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table['common_name'] = reference_table['common_name'].str.split('/').str[-1].str.lstrip().str.rstrip().str.replace('.',\"\").str.replace(\"'\",\"\").str.replace(\"-\",\" \")\n",
    "reference_table['scientific_name'] = reference_table['scientific_name'].str.split('/').str[-1].str.lstrip().str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_table.to_csv('../dataset/wildfooduk_mapping_table.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross analysing base dataset and complementary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/observations_mushroom.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match_count = 0\n",
    "print(\"species not present in the initial dataset\")\n",
    "print(\"-------------------------------------------\")   \n",
    "\n",
    "for element in reference_table['scientific_name'].unique():\n",
    "    if element in df['label'].unique():\n",
    "        match_count = match_count + 1\n",
    "    else:\n",
    "        print(element)\n",
    "\n",
    "print(\"-------------------------------------------\")        \n",
    "print(\"matching species count: \" + str(match_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding identified species into the edible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edible mushrooms of the imported dataset will be filtered to be added in the edible dataset\n",
    "# images will get a unique identifier not already used by the existing dataset starting from 1 000 000 for clarity\n",
    "# only the scientific name has been deduced. The reste of the data frame needs also to be filled\n",
    "# images are converted to jpeg for homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edible = pd.read_csv('../dataset/edible_mushrooms.csv')\n",
    "df_edible.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = df_edible.drop(columns = ['image_lien', 'image_id'])\n",
    "df_classes = df_classes[[ 'species','phylum', 'class', 'order', 'family','genus']].drop_duplicates()\n",
    "df_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling exceptions:\n",
    "name_exceptions = {\n",
    "\"blackening polypore\" : \"giant polypore\",\n",
    "\"cauliflower fungus\" : \"wood cauliflower\",\n",
    "\"clouded agaric\" : \"clouded funnel\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviewing possibles matches : common name, synonyms, and scientific name used as a common name\n",
    "\n",
    "def find_scientific_name(common_name, reference_table, name_exceptions):\n",
    "    try:\n",
    "        scientific_name = reference_table[reference_table['common_name'].str.lower() == common_name]['scientific_name'].iloc[0]\n",
    "    except:\n",
    "        try:\n",
    "            scientific_name = reference_table[reference_table['scientific_name'].str.lower() == common_name]['scientific_name'].iloc[0]\n",
    "        except:\n",
    "            try:\n",
    "                synonym = name_exceptions[common_name.lower()]\n",
    "                scientific_name = reference_table[reference_table['common_name'].str.lower() == synonym]['scientific_name'].iloc[0]\n",
    "            except:\n",
    "                print( \"not found : \" + common_name)   \n",
    "                scientific_name = \"\"\n",
    "    return scientific_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_page(mushroom_name):\n",
    "\n",
    "    # Preparing classification\n",
    "    classification = dict()\n",
    "    classification[\"species\"] = mushroom_name\n",
    "\n",
    "    # Replace spaces in the mushroom name with underscores to match Wikipedia's URL format\n",
    "    mushroom_name = mushroom_name.replace(' ', '_')\n",
    "\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(f\"https://en.wikipedia.org/wiki/{mushroom_name}\")\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get page: {response.status_code} \" + \" for \" + mushroom_name)\n",
    "        return\n",
    "\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the title of the page\n",
    "    title = soup.find(id=\"firstHeading\")\n",
    "\n",
    "    # Print the title\n",
    "    #print(title.string)\n",
    "\n",
    "    # Find the table with the scientific classification\n",
    "    rows = soup.find_all('td')\n",
    "\n",
    "    # Iterate over each row\n",
    "    iterator = iter(rows)\n",
    "    for row in iterator:\n",
    "        #print(repr(row.text.replace(\":\",\"\").rstrip()))\n",
    "        if(row.text.replace(\":\",\"\").rstrip() == \"Division\"):\n",
    "            classification[\"phylum\"] = next(iterator).text.strip()\n",
    "\n",
    "        if(row.text.replace(\":\",\"\").rstrip() == \"Class\"):\n",
    "            classification[\"class\"] = next(iterator).text.strip()\n",
    "        \n",
    "        if(row.text.replace(\":\",\"\").rstrip() == \"Order\"):\n",
    "            classification[\"order\"] = next(iterator).text.strip()\n",
    "\n",
    "        if(row.text.replace(\":\",\"\").rstrip() == \"Family\"):\n",
    "            classification[\"family\"] = next(iterator).text.strip()\n",
    "\n",
    "        if(row.text.replace(\":\",\"\").rstrip() == \"Genus\"):\n",
    "            classification[\"genus\"] = next(iterator).text.strip()\n",
    "\n",
    "    # Check for edibility\n",
    "    edibility = soup.find('a', {'href': '/wiki/Edible_mushroom'})\n",
    "    classification[\"edible\"] = 1 if (edibility is not None) else 0\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking genus if species is not found in the main dataframeb\n",
    "not_found_cnt = 0\n",
    "species_cnt = 0\n",
    "classification_list = []\n",
    "wiki_match_cnt = 0\n",
    "species_match = 0\n",
    "\n",
    "for dir_name, subdir_list, file_list in os.walk(os.path.join(dataset_path, 'data', 'data')):\n",
    "    species_cnt = species_cnt + 1  \n",
    "\n",
    "    # getting common name from subdirectory name       \n",
    "    common_name = dir_name.split(os.sep)[-1].replace('_', ' ')\n",
    "    scientific_name = find_scientific_name(common_name, reference_table,name_exceptions)\n",
    "\n",
    "    # getting information related to the species\n",
    "    try:\n",
    "        scientific_classification = df_classes[df_classes[\"species\"].str.lower() == scientific_name.lower()].iloc[0]\n",
    "        species_match = species_match + 1\n",
    "        # no common name is stored at the moment, which is required to build the test dataset\n",
    "        classification = scientific_classification.to_dict()\n",
    "        classification['common_name'] = common_name\n",
    "        classification_list.append(classification)       \n",
    "    except:\n",
    "        try:\n",
    "            # in this case the  classification from wikipedia is used\n",
    "            classification = scrape_wikipedia_page(scientific_name)\n",
    "            \n",
    "            # species is added only if edible\n",
    "            if classification['edible'] == 1:\n",
    "                classification['common_name'] = common_name\n",
    "                classification_list.append(classification)\n",
    "\n",
    "            wiki_match_cnt = wiki_match_cnt + 1\n",
    "        except:\n",
    "            print(\"not found : \" + str(species_cnt) + \" \" + scientific_name)\n",
    "            not_found_cnt = not_found_cnt + 1\n",
    "\n",
    "print(\"species match: \" + str(species_match) + \" wiki match: \" + str(wiki_match_cnt) + \" not found: \" + str(not_found_cnt) +  \" total species: \" + str(species_cnt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those which don't have a matching wikipedia page are completely removed\n",
    "filtered_list = [d for d in classification_list if (d is not None) ]\n",
    "\n",
    "# now we can update the classification dataframe we the newly identified species. \n",
    "df_classes_update = pd.DataFrame(filtered_list)\n",
    "df_classes_update = df_classes_update.dropna(axis=0)\n",
    "df_classes_update = df_classes_update.drop(columns=['edible'])\n",
    "df_classes_update.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_augmented = pd.concat([df_classes, df_classes_update],ignore_index=True)\n",
    "df_class_augmented.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the order test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming and converting to jpg all images with a unique identifier in their source directory\n",
    "cnt_img = 0\n",
    "start_index = 1000000\n",
    "\n",
    "for dir_name, subdir_list, file_list in os.walk(os.path.join(dataset_path, 'data', 'data')):\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".png\"):\n",
    "            # create unique identifier\n",
    "            unique_identifier  = str(start_index + cnt_img)\n",
    "            cnt_img = cnt_img + 1\n",
    "\n",
    "            # Get the full path of the file\n",
    "            old_file_path = os.path.join(dir_name, file_name)\n",
    "\n",
    "            # Construct the new file name with the unique identifier\n",
    "            new_file_name = unique_identifier + '.jpg'\n",
    "\n",
    "            # Construct the new full path with the new file name\n",
    "            new_file_path = os.path.join(dir_name, new_file_name)\n",
    "\n",
    "            # Open and convert the PNG image to JPG using Pillow\n",
    "            image = Image.open(old_file_path)\n",
    "            image = image.convert(\"RGB\")\n",
    "            image.save(new_file_path, \"JPEG\")\n",
    "\n",
    "            # Remove the old PNG file\n",
    "            os.remove(old_file_path)\n",
    "\n",
    "            print(f\"Converted '{old_file_path}' to '{new_file_path}'\")    \n",
    "            \n",
    "\n",
    "print(cnt_img)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datatset_path = '../dataset/order/validation'\n",
    "\n",
    "# Create the folder silently\n",
    "os.makedirs(validation_datatset_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the subdirectory name, common name is extracted and matched to its order\n",
    "not_found_cnt = 0\n",
    "rootdir = os.path.join(dataset_path, 'data', 'data')\n",
    "\n",
    "# Get a list of all subdirectories\n",
    "subdirectories = [d for d in os.listdir(rootdir)]\n",
    "\n",
    "# Create a dictionary to store the mapping of subdirectories to orders\n",
    "subdir_to_order = {}\n",
    "\n",
    "# Iterate through subdirectories and match with 'common_name' or 'species'\n",
    "for subdir in subdirectories:\n",
    "    common_name = subdir.replace('_', ' ')\n",
    "\n",
    "    scientific_name = find_scientific_name(common_name, df_classes_update.rename(columns={'species': 'scientific_name'}),name_exceptions)\n",
    "\n",
    "    matching_species = df_classes_update[df_classes_update['species'] == scientific_name]\n",
    "\n",
    "    if not matching_species.empty:\n",
    "        order_value = matching_species.iloc[0]['order']\n",
    "        subdir_to_order[subdir] = order_value\n",
    "\n",
    "        # Copy the source folder and its contents to the destination folder\n",
    "        shutil.copytree(os.path.join(rootdir, subdir), os.path.join(validation_datatset_path, subdir))\n",
    "    else: \n",
    "        # at that stage not found means not edible\n",
    "        not_found_cnt = not_found_cnt + 1\n",
    "\n",
    "print(\"found: \" + str(len(subdirectories) - not_found_cnt) + \" / \" + str(df_classes_update['species'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
