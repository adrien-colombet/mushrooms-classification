{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbdf9df",
   "metadata": {},
   "source": [
    "# Exploration de différents CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a67154",
   "metadata": {},
   "source": [
    "L'objectif de ce notebook est d'entrainer et évaluer différents modeles de classification de type CNN ou RNN à déterminer si un champignon et comestible ou pas à partir d'une image.\n",
    "Les inputs de ce notebook sont :\n",
    "- le dataset d'images nettoyé et le fichier .csv correspondant au dataset d'images qui sera utilisé pour les parties train et test,\n",
    "- le dataset de validation avec le fichier .csv associé.\n",
    "\n",
    "Une visualisation durant l'apprentissage sera réalisée par le biais de GradCam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dc663",
   "metadata": {},
   "source": [
    "Ce notebook est inspiré du travail de HOA NGUYEN (https://www.kaggle.com/code/nguyenhoa/dog-cat-classifier-gradcam-with-tensorflow-2-0/notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import  keras\n",
    "import tensorflow as tf # Utilisation de tensorflow v2.9.1\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création des liens vers les dossiers et fichiers source\n",
    "images_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\FFD_images_dataset'\n",
    "train_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\train_FFDataframe_full_undersampling.csv'\n",
    "test_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\test_FFDataframe_full_undersampling.csv'\n",
    "validation_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\val_FFDataframe_full.csv'\n",
    "\n",
    "# dossier ou sauver les résultats obtenus sur les modèles\n",
    "save_models_results = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\Models_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de quelques fonctions utiles\n",
    "\n",
    "# affichage des metriques (accuracy, loss) d'entrainement d'un modèle\n",
    "def plot_scores(model, title):\n",
    "    '''\n",
    "    Arg :\n",
    "    model : model dont on souhaite afficher les metriques\n",
    "    Return:\n",
    "    plot des métriques Accuracy et loss sur les datasets train et test\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.rcParams['figure.figsize'] = [14,4]\n",
    "\n",
    "    # Créer la figure\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.gcf().subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0.3, hspace = 0.3)\n",
    "    # Créer les 4 graphiques\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # Tracer les données sur les graphiques\n",
    "    ax1.plot(model.history['accuracy'], label = \"train\")\n",
    "    ax1.plot(model.history['val_accuracy'], label = \"test\")\n",
    "    ax1.legend(loc = \"lower right\")\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')    \n",
    "\n",
    "    ax2.plot(model.history['loss'], label = \"train\")\n",
    "    ax2.plot(model.history['val_loss'], label = \"test\")\n",
    "    ax2.legend(loc = \"upper right\")\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')  \n",
    "    plt.title(title, loc = \"left\")\n",
    "    plt.show()\n",
    "    \n",
    "# affichage de la matrice de confusion du dataset de validation\n",
    "def show_confusion_matrix(model):\n",
    "    '''\n",
    "    Args :\n",
    "    model : modele à utiliser pour fair eles predictions\n",
    "   \n",
    "    Return :\n",
    "    plot de la matrice de confusion\n",
    "    '''\n",
    "    # réalisation des prédiction pour le modèle\n",
    "    model_pred=model.predict(val_generator, steps=val_steps, verbose=1)\n",
    "    y_pred = []\n",
    "    for element in model_pred:\n",
    "        pred = np.argmax(element)\n",
    "        y_pred.append(pred)\n",
    "    y_val = df_val.edible.to_list()\n",
    "    confusion_mtx = confusion_matrix(y_val, y_pred)\n",
    "    #\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mtx)\n",
    "    disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal', colorbar = False)\n",
    "    plt.title(f'Confusion matrix for {model}')\n",
    "    plt.ylabel('True label', fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.xlabel('Predicted label', fontsize = 20)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des dataframes\n",
    "df_train = pd.read_csv(train_dataset)\n",
    "df_test = pd.read_csv(test_dataset)\n",
    "df_val = pd.read_csv(validation_dataset)\n",
    "\n",
    "# affichage de quelques infos sur ces dataframes + affichage d'une figure de répartition des catégories\n",
    "display(df_train.head(), df_test.info(), df_val.info())\n",
    "\n",
    "# génération des données du graph\n",
    "inedible = []\n",
    "edible = []\n",
    "\n",
    "dataframes = [df_train, df_test, df_val]\n",
    "for dataframe in dataframes:\n",
    "    count_inedible = dataframe['edible'].value_counts()[0]\n",
    "    inedible.append(count_inedible)\n",
    "    count_edible = dataframe['edible'].value_counts()[1]\n",
    "    edible.append(count_edible)   \n",
    "\n",
    "data = ['df_train', 'df_test', 'df_val']\n",
    "edibility = {'inedible': inedible, 'edible' : edible}\n",
    "\n",
    "colonnes = ['df_train', 'df_test', 'df_val']\n",
    "sex_counts = {\n",
    "    'inedible': inedible,\n",
    "    'edible': edible\n",
    "}\n",
    "\n",
    "width = 0.6\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "for i, j in edibility.items():\n",
    "    p = ax.bar(data, j, width, label=i, bottom=bottom)\n",
    "    bottom += j\n",
    "    ax.bar_label(p, label_type='center')\n",
    "ax.set_title('Number of images by category')\n",
    "ax.legend(title = 'categories')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e438af",
   "metadata": {},
   "source": [
    "## Définition des classes GradCAM & GuidedGradCAM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9f8de",
   "metadata": {},
   "source": [
    "### GradCAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    # Adapté avec quelques modifications de  https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
    "    def __init__(self, model, layerName=None):\n",
    "        \"\"\"\n",
    "        model: pre-softmax layer (logit layer)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "            \n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "    \n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM\")\n",
    "            \n",
    "    def compute_heatmap(self, image, classIdx, upsample_size, eps=1e-5):\n",
    "        gradModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output, self.model.output]\n",
    "        )\n",
    "        # record operations for automatic differentiation\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOuts, preds) = gradModel(inputs) # preds after softmax\n",
    "            loss = preds[:,classIdx]\n",
    "        \n",
    "        # compute gradients with automatic differentiation\n",
    "        grads = tape.gradient(loss, convOuts)\n",
    "        # discard batch\n",
    "        convOuts = convOuts[0]\n",
    "        grads = grads[0]\n",
    "        norm_grads = tf.divide(grads, tf.reduce_mean(tf.square(grads)) + tf.constant(eps))\n",
    "        \n",
    "        # compute weights\n",
    "        weights = tf.reduce_mean(norm_grads, axis=(0,1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOuts), axis=-1)\n",
    "        \n",
    "        # Apply reLU\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam/np.max(cam)\n",
    "        cam = cv2.resize(cam, upsample_size,interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # convert to 3D\n",
    "        cam3 = np.expand_dims(cam, axis=2)\n",
    "        cam3 = np.tile(cam3, [1,1,3])\n",
    "        \n",
    "        return cam3\n",
    "    \n",
    "def overlay_gradCAM(img, cam3):\n",
    "    cam3 = np.uint8(255*cam3)\n",
    "    cam3 = cv2.applyColorMap(cam3, cv2.COLORMAP_JET)\n",
    "    \n",
    "    new_img = 0.3*cam3 + 0.5*img\n",
    "    \n",
    "    return (new_img*255.0/new_img.max()).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90762fc3",
   "metadata": {},
   "source": [
    "### Guided_GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def guidedRelu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
    "    return tf.nn.relu(x), grad\n",
    "\n",
    "# Reference: https://github.com/eclique/keras-gradcam with adaption to tensorflow 2.0  \n",
    "class GuidedBackprop:\n",
    "    def __init__(self,model, layerName=None):\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "        self.gbModel = self.build_guided_model()\n",
    "        \n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply Guided Backpropagation\")\n",
    "\n",
    "    def build_guided_model(self):\n",
    "        gbModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output]\n",
    "        )\n",
    "        layer_dict = [layer for layer in gbModel.layers[1:] if hasattr(layer,\"activation\")]\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == tf.keras.activations.relu:\n",
    "                layer.activation = guidedRelu\n",
    "        \n",
    "        return gbModel\n",
    "    \n",
    "    def guided_backprop(self, images, upsample_size):\n",
    "        \"\"\"Guided Backpropagation method for visualizing input saliency.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(images, tf.float32)\n",
    "            tape.watch(inputs)\n",
    "            outputs = self.gbModel(inputs)\n",
    "\n",
    "        grads = tape.gradient(outputs, inputs)[0]\n",
    "\n",
    "        saliency = cv2.resize(np.asarray(grads), upsample_size)\n",
    "\n",
    "        return saliency\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Same normalization as in:\n",
    "    # https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    \n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x = x.copy()\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e84ff",
   "metadata": {},
   "source": [
    "### Creation d'une fonction de visualisation des GraCAM et Guided_GradCAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gradCAMs(model, gradCAM, GuidedBP, im_ls, n=3, decode={}):\n",
    "    \"\"\"\n",
    "    model: softmax layer\n",
    "    \"\"\"\n",
    "    random.shuffle(im_ls)\n",
    "    plt.subplots(figsize=(30, 10*n))\n",
    "    k=1\n",
    "    for i in range(n):\n",
    "        img = cv2.imread(os.path.join(images_dataset,im_ls[i]))\n",
    "        upsample_size = (img.shape[1],img.shape[0])\n",
    "        if (i+1) == len(df_test):\n",
    "            break\n",
    "        # Show original image\n",
    "        plt.subplot(n,3,k)\n",
    "        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Filename: {}\".format(im_ls[i]), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        # Show overlayed grad\n",
    "        plt.subplot(n,3,k+1)\n",
    "        im = img_to_array(load_img(os.path.join(images_dataset,im_ls[i]), target_size=(W,H)))\n",
    "        x = np.expand_dims(im, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        preds = model.predict(x)\n",
    "        idx = preds.argmax()\n",
    "        if len(decode)==0:\n",
    "            res = decode_predictions(preds)[0][0][1:]\n",
    "        else:\n",
    "            res = [decode[idx],preds.max()]\n",
    "        cam3 = gradCAM.compute_heatmap(image=x, classIdx=idx, upsample_size=upsample_size)\n",
    "        new_img = overlay_gradCAM(img, cam3)\n",
    "        new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(new_img)\n",
    "        plt.title(\"GradCAM - Pred: {}. Prob: {}\".format(res[0],res[1]), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Show guided GradCAM\n",
    "        plt.subplot(n,3,k+2)\n",
    "        gb = GuidedBP.guided_backprop(x, upsample_size)\n",
    "        guided_gradcam = deprocess_image(gb*cam3)\n",
    "        guided_gradcam = cv2.cvtColor(guided_gradcam, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(guided_gradcam)\n",
    "        plt.title(\"Guided GradCAM\", fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        k += 3\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c131a3b",
   "metadata": {},
   "source": [
    "## Mise en place de différents modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac83ed",
   "metadata": {},
   "source": [
    "### Création de générateurs de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0127f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataGenerator pour les datasets d'entrainement et de test spécifique pou rle modèle simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a38425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f69ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a98c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de quelques paramètres\n",
    "batch_size = 64\n",
    "SEED = 3\n",
    "epochs = 15\n",
    "W, H = 224, 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset d'entrainement\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_train[\"edible\"] = df_train[\"edible\"].apply(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(df_train, images_dataset,\n",
    "                                                    x_col=\"filename\",\n",
    "                                                    y_col=\"edible\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation du fonctionnement du data generator sur quelques images du dataset d'entrainement\n",
    "ex_df = df_train.sample(n=15).reset_index(drop=True)\n",
    "ex_gen = train_datagen.flow_from_dataframe(ex_df,images_dataset,\n",
    "                                           x_col=\"filename\",\n",
    "                                           y_col=\"edible\",\n",
    "                                           class_mode=\"categorical\")\n",
    "\n",
    "# affichage de quelquyes images issues du générateur\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    for x, y in ex_gen:\n",
    "        im = x[0]\n",
    "        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset de test\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_test[\"edible\"] = df_test[\"edible\"].apply(str)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(df_test, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  y_col=\"edible\",\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf663946",
   "metadata": {},
   "source": [
    "### Modèle de référence : Création d'un premier CNN simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3167fe",
   "metadata": {},
   "source": [
    "Un premier CNN d'architecture simple est créé afin d'obtenir des résulats sur un modèle de référence. Ce réseau est constitué de 2 couches de convolutions puis 2 couches Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un modèle simple\n",
    "simple_model = tf.keras.Sequential(name = 'simple_model')\n",
    "# block1\n",
    "simple_model.add(Conv2D(filters = 64,                    # Nombre de filtres\n",
    "                        kernel_size = (3,3),             # Shape du kernel\n",
    "                        input_shape = (224, 224, 3),     # Shape de l'entrée\n",
    "                        activation = 'relu',             # Fonction d'activation\n",
    "                        name = 'conv1_block1_out'))      # nom de la couche\n",
    "simple_model.add(MaxPooling2D(pool_size = (2, 2),\n",
    "                              strides = 2,\n",
    "                              name = 'maxpo1_block1_out'))\n",
    "simple_model.add(Conv2D(filters = 128,                    \n",
    "                        kernel_size = (3,3),\n",
    "                        activation = 'relu',\n",
    "                        name = 'conv2_block1_out'))\n",
    "simple_model.add(MaxPooling2D(pool_size = (2, 2),\n",
    "                              strides = 2,\n",
    "                              name = 'maxpo2_block1_out'))\n",
    "\n",
    "# block2\n",
    "\n",
    "simple_model.add(Flatten(name = 'flat1_block2_out'))\n",
    "simple_model.add(Dropout(rate = 0.2,\n",
    "                         name = 'drop1_block2_out'))\n",
    "\n",
    "\n",
    "#block3\n",
    "simple_model.add(Dense(units = 128,\n",
    "                       activation = 'relu',\n",
    "                       name = 'den1_block3_out'))\n",
    "simple_model.add(Dense(units = 2,\n",
    "                       activation = 'softmax',\n",
    "                       name = 'den2_block3_out'))\n",
    "\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319069b6",
   "metadata": {},
   "source": [
    "### Transfert learning de différents CNN pré-entrainés "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766a1e1",
   "metadata": {},
   "source": [
    "Le modèle simple sera comparés avec différentes architectures : EfficientNetB0, VGG19, Xception, ResNet50V2, InceptionV3.\n",
    "Ces différents modèle seront instanciés avec les poids de l'entrainement obtenu sur le dataset imagenet. La dernière couche Dense sera convertie pour obtenir une sortie à 2 outputs avec une activation softmax pour renvoyer une probabilité d'appartenance à chacun des outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28e4fe",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c38142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type EfficientNetB0 à deux sorties\n",
    "TL_EfficientNetB0 = EfficientNetB0(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_EfficientNetB0.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_EfficientNetB0.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_EfficientNetB0 = Model(TL_EfficientNetB0.input, output, name = 'EfficientNetB0')\n",
    "TL_EfficientNetB0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda9e3e",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type VGG19 à deux sorties\n",
    "TL_VGG19 = VGG19(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_VGG19.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_VGG19.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_VGG19 = Model(TL_VGG19.input, output, name = 'TL_VGG19')\n",
    "TL_VGG19.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf15ca6",
   "metadata": {},
   "source": [
    "#### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type Xception à deux sorties\n",
    "TL_Xception = Xception(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_Xception.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_Xception.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_Xception = Model(TL_Xception.input, output, name = 'TL_Xception')\n",
    "TL_Xception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca88f19",
   "metadata": {},
   "source": [
    "#### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type ResNet50V2 à deux sorties\n",
    "TL_ResNet50V2 = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_ResNet50V2.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_ResNet50V2.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_ResNet50V2 = Model(TL_ResNet50V2.input, output, name = 'TL_ResNet50V2')\n",
    "TL_ResNet50V2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a7112",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type InceptionV3 à deux sorties\n",
    "TL_InceptionV3 = InceptionV3(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_InceptionV3.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_InceptionV3.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_InceptionV3 = Model(TL_InceptionV3.input, output, name = 'TL_InceptionV3')\n",
    "TL_InceptionV3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52863c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'une fonction permettant de compiler un modèle\n",
    "def compile_model(model, optimizer, loss, metrics):\n",
    "    '''\n",
    "    Args :\n",
    "    model : model à compiler\n",
    "    optimizer :  choix de l'optimizer à utiliser durant l'entrainement\n",
    "    loss : fonction de loss à utiliser durant l'entrainement sous la forme : \"loss\"\n",
    "    metrics : metrique à évaluer durant l'entrainement sou sla forme : [\"metrics\"]\n",
    "    '''\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des paramètres à compiler pour l'entrainement\n",
    "optimizer = optimizers.SGD()\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "# compilation des modèles\n",
    "compile_model(simple_model, optimizer, loss, metrics)\n",
    "compile_model(TL_EfficientNetB0, optimizer, loss, metrics)\n",
    "compile_model(TL_VGG19, optimizer, loss, metrics)\n",
    "compile_model(TL_Xception, optimizer, loss, metrics)\n",
    "compile_model(TL_ResNet50V2, optimizer, loss, metrics)\n",
    "compile_model(TL_InceptionV3, optimizer, loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de callbacks\n",
    "# pour arreter l'entrainement si la metric val_loss n'évolue pas aprés 3 epochs\n",
    "earlystoper = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# pour sauvegarder les modèles durant l'entrainement\n",
    "checkpointer_simple_model = ModelCheckpoint(filepath=os.path.join(save_models_results, \"simple_model_best.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "checkpointer_TL_EfficientNetB0 = ModelCheckpoint(filepath=os.path.join(save_models_results, \"TL_EfficientNetB0.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "checkpointer_TL_VGG19 = ModelCheckpoint(filepath=os.path.join(save_models_results, \"TL_VGG19.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "checkpointer_TL_Xception = ModelCheckpoint(filepath=os.path.join(save_models_results, \"TL_Xception.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "checkpointer_TL_ResNet50V2 = ModelCheckpoint(filepath=os.path.join(save_models_results, \"TL_ResNet50V2.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "checkpointer_TL_InceptionV3 = ModelCheckpoint(filepath=os.path.join(save_models_results, \"TL_InceptionV3.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "\n",
    "# pour sauvegarder les métriques durant l'entrainement\n",
    "CSV_logger_simple_model = CSVLogger(filename = 'logger_simple_model.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "CSV_logger_TL_EfficientNetB0 = CSVLogger(filename = 'logger_TL_EfficientNetB0.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "CSV_logger_TL_VGG19 = CSVLogger(filename = 'logger_TL_VGG19.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "CSV_logger_TL_Xception = CSVLogger(filename = 'logger_TL_Xception.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "CSV_logger_TL_ResNet50V2 = CSVLogger(filename = 'logger_TL_ResNet50V2.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "CSV_logger_TL_InceptionV3 = CSVLogger(filename = 'logger_TL_InceptionV3.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "\n",
    "# création des callbacks\n",
    "callbacks_simple_model = [earlystoper, checkpointer_simple_model, CSV_logger_simple_model]\n",
    "callbacks_TL_EfficientNetB0 = [earlystoper, checkpointer_TL_EfficientNetB0, CSV_logger_TL_EfficientNetB0]\n",
    "callbacks_TL_VGG19 = [earlystoper, checkpointer_TL_VGG19, CSV_logger_TL_VGG19]\n",
    "callbacks_TL_Xception = [earlystoper, checkpointer_TL_Xception, CSV_logger_TL_Xception]\n",
    "callbacks_TL_ResNet50V2 = [earlystoper, checkpointer_TL_ResNet50V2, CSV_logger_TL_ResNet50V2]\n",
    "callbacks_TL_InceptionV3 = [earlystoper, checkpointer_TL_InceptionV3, CSV_logger_TL_InceptionV3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'une fonction permettant l'entrainement de modéle\n",
    "def train_model(model, train_generator, val_generator, epochs, batch_size, callbacks):\n",
    "    '''\n",
    "    Args :\n",
    "    model : model à entrainer\n",
    "    train_generator : dataset d'entrainement\n",
    "    val_generator : dataset de test a utiliser pour ajuster les poids du modele aprés chaque epoch\n",
    "    epochs : nombre d'epochs d'entrainement\n",
    "    batch_size : dimension des batchs d'images durant l'entrainement\n",
    "    callbacks : liste des callbacks à utiliser\n",
    "    '''\n",
    "    # Entraîner le modèle\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=len(val_df_X) // batch_size,\n",
    "                                  steps_per_epoch=len(train_df_X) // batch_size,\n",
    "                                  callbacks=callbacks)\n",
    "    # Générer le nom de la variable history\n",
    "    name = \"history_\" + model.name + \"_\" + str(epochs) + \"_\" + str(batch_size)\n",
    "    # Retourner le modèle et l'historique\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ca70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche de possibilité d'utilisation d'une GPU\n",
    "print('GPU Available : ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f53fd4",
   "metadata": {},
   "source": [
    "### Entrianements des modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22322ff4",
   "metadata": {},
   "source": [
    "#### Modèle simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle simple\n",
    "batch_size_simple_model = 64\n",
    "start_time = time.time()\n",
    "history_simple_model = simple_model.fit(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size_simple_model,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size_simple_model,\n",
    "                                                    callbacks=callbacks_simple_model)\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e29f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(simple_model, \"simple_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd319c",
   "metadata": {},
   "source": [
    "#### EfficientNetB0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle TL_EfficientNetB0\n",
    "start_time = time.time()\n",
    "history_TL_EfficientNetB0 = TL_EfficientNetB0.fit(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_TL_EfficientNetB0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a720dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_TL_EfficientNetB0, \"TL_EfficientNetB0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa79d3a",
   "metadata": {},
   "source": [
    "#### VGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle TL_VGG19\n",
    "start_time = time.time()\n",
    "history_TL_VGG19 = TL_VGG19.fit_generator(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_TL_VGG19)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_TL_VGG19, \"TL_VGG19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e697d",
   "metadata": {},
   "source": [
    "#### Xception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b490c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle TL_Xception\n",
    "start_time = time.time()\n",
    "history_TL_Xception = TL_Xception.fit_generator(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_TL_Xception)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61980841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_TL_Xception, \"TL_Xception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5741723",
   "metadata": {},
   "source": [
    "#### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du TL_ResNet50V2\n",
    "start_time = time.time()\n",
    "history_TL_ResNet50V2 = TL_ResNet50V2.fit_generator(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_TL_ResNet50V2)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fa6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_TL_ResNet50V2, \"TL_ResNet50V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38814cc1",
   "metadata": {},
   "source": [
    "#### InceptionV3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du TL_InceptionV3\n",
    "start_time = time.time()\n",
    "history_TL_InceptionV3 = TL_InceptionV3.fit_generator(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_TL_InceptionV3)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_TL_InceptionV3, \"TL_InceptionV3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d9dba",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "Les courbes d'apprentissage des différents modèles indiquent que les modèles les plus prometteurs sont VGG19, Xception, ResNet50V2 et InceptionV3. L'apprentissage pour le modèle ResNet50V2 semble néamoins stagner rapidement tandis qu'elle est moins marquée sur les trois autres modèles. EfficientNetB0 présente les plus mauvais résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6d7a2",
   "metadata": {},
   "source": [
    "## Affichage des GradCAM et Guided_GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7688309",
   "metadata": {},
   "source": [
    "L'objectif est ici d'évaluer de quelle manière les couches de convolutions des différents modèles permettent d'extraire l'information en vue de la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcb499",
   "metadata": {},
   "source": [
    "#### Modèle simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f1d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1aaca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d406ee88",
   "metadata": {},
   "source": [
    "#### EfficientNetB0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8366aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques GradCAM et Guided GradCAM pour le modele TL_EfficientNetB0\n",
    "\n",
    "TL_EfficientNetB0_logit = Model(TL_EfficientNetB0.input,TL_EfficientNetB0.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_TL_EfficientNetB0 = GradCAM(model=TL_EfficientNetB0, layerName=\"top_conv\")\n",
    "retrained_guidedBP_TL_EfficientNetB0 = GuidedBackprop(model=TL_EfficientNetB0, layerName=\"top_conv\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(df_test, images_dataset, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = TL_EfficientNetB0.predict(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = df_test.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.edible == \"1\") & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.edible == \"0\") & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "show_gradCAMs(TL_EfficientNetB0, retrained_gradCAM_TL_EfficientNetB0, retrained_guidedBP_TL_EfficientNetB0, true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b5838",
   "metadata": {},
   "source": [
    "Les GradCAM et GuidedGradCam affichées pour 5 exemples de champignons comestibles du dataset de test obtenues avec le modèle EfficientNetB0 indiquent que le modèle se focalise mal sur le champignon lui même mais plutot sur le centre de l'image sans faire de distinction entre le champignon ou d'autres objets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0397c6",
   "metadata": {},
   "source": [
    "#### VGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90067a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques GradCAM et Guided GradCAM pour le modele TL_VGG19\n",
    "\n",
    "TL_VGG19_logit = Model(TL_VGG19.input,TL_VGG19.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_TL_VGG19 = GradCAM(model=TL_VGG19, layerName=\"block5_conv4\")\n",
    "retrained_guidedBP_TL_VGG19 = GuidedBackprop(model=TL_VGG19, layerName=\"block5_conv4\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(df_test, images_dataset, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = TL_VGG19.predict(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = df_test.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.edible == \"1\") & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.edible == \"0\") & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "show_gradCAMs(TL_VGG19, retrained_gradCAM_TL_VGG19, retrained_guidedBP_TL_VGG19, true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f657ec",
   "metadata": {},
   "source": [
    "Les GradCAM et GuidedGradCam affichées pour 5 exemples de champignons comestibles du dataset de test obtenues avec le modèle VGG19 indiquent que le modèle se focalise bien sur le champignon, les couches de convolutions successives permttent d'extraire correctement l'information à priori voulue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d525780",
   "metadata": {},
   "source": [
    "#### Xception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques GradCAM et Guided GradCAM pour le modele TL_Xception\n",
    "\n",
    "TL_Xception_logit = Model(TL_Xception.input,TL_Xception.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_TL_Xception = GradCAM(model=TL_Xception, layerName=\"block14_sepconv2\")\n",
    "retrained_guidedBP_TL_Xception = GuidedBackprop(model=TL_Xception, layerName=\"block14_sepconv2\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(df_test, images_dataset, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = TL_Xception.predict(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = df_test.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.edible == \"1\") & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.edible == \"0\") & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "show_gradCAMs(TL_Xception, retrained_gradCAM_TL_Xception, retrained_guidedBP_TL_Xception, true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e93d20",
   "metadata": {},
   "source": [
    "Les GradCAM et GuidedGradCam affichées pour 5 exemples de champignons comestibles du dataset de test obtenues avec le modèle Xception indiquent que le modèle se focalise bien sur le champignon, les couches de convolutions successives permttent d'extraire correctement l'information à priori voulue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3e739",
   "metadata": {},
   "source": [
    "#### ResNet50V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques GradCAM et Guided GradCAM pour le modele TL_ResNet50V2\n",
    "\n",
    "TL_ResNet50V2_logit = Model(TL_ResNet50V2.input,TL_ResNet50V2.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_TL_ResNet50V2 = GradCAM(model=TL_ResNet50V2, layerName=\"conv5_block3_3_conv\")\n",
    "retrained_guidedBP_TL_ResNet50V2 = GuidedBackprop(model=TL_ResNet50V2, layerName=\"conv5_block3_3_conv\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(df_test, images_dataset, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = TL_ResNet50V2.predict(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = df_test.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.edible == \"1\") & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.edible == \"0\") & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "show_gradCAMs(TL_ResNet50V2, retrained_gradCAM_TL_ResNet50V2, retrained_guidedBP_TL_ResNet50V2, true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf1fb5",
   "metadata": {},
   "source": [
    "Les GradCAM et GuidedGradCam affichées pour 5 exemples de champignons comestibles du dataset de test obtenues avec le modèle ResNet50V2 indiquent que le modèle se focalise trés bien sur le champignon, les couches de convolutions successives permttent d'extraire correctement l'information à priori voulue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ae683",
   "metadata": {},
   "source": [
    "#### InceptionV3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91362c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques GradCAM et Guided GradCAM pour le modele TL_InceptionV3\n",
    "\n",
    "TL_InceptionV3_logit = Model(TL_InceptionV3.input,TL_InceptionV3.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_TL_InceptionV3 = GradCAM(model=TL_InceptionV3, layerName=\"conv2d_195\")\n",
    "retrained_guidedBP_TL_InceptionV3 = GuidedBackprop(model=TL_InceptionV3, layerName=\"conv2d_195\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(df_test, images_dataset, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = TL_InceptionV3.predict(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = df_test.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.edible == \"1\") & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.edible == \"0\") & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "show_gradCAMs(TL_InceptionV3, retrained_gradCAM_TL_InceptionV3, retrained_guidedBP_TL_InceptionV3, true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f476ba4",
   "metadata": {},
   "source": [
    "Les GradCAM et GuidedGradCam affichées pour 5 exemples de champignons comestibles du dataset de test obtenues avec le modèle InceptionV3 indiquent que le modèle se focalise trés bien sur le champignon, les couches de convolutions successives permttent d'extraire correctement l'information à priori voulue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc6afb",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "L'observation des GradCAM et GuidedGradCAM laisse suggérer que les modèles ResNet50V2 et InceptionV3 extraits le plus d'informations des images. Le modèle EfficienNetB0 semble le moins performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50ba34",
   "metadata": {},
   "source": [
    "## Prédictions sur le dataset de validation et matrices de confusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32515550",
   "metadata": {},
   "source": [
    "Aprés avoir évaluer les différents modèles à travers leurs apprentissages respectif, nous allons chercher à évaluer le ou lesquels obtiennent les meilleurs résultats sur le dataset de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset de validation\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(df_val, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  class_mode=None,\n",
    "                                                  batch_size=1)\n",
    "val_steps = len(df_val)\n",
    "df_val[\"edible\"] = df_val[\"edible\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3800fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rappel du nombre d'images dans les différentes catégories pour le dataset de validation\n",
    "df_val.edible.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c38f05",
   "metadata": {},
   "source": [
    "#### Modèle simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5afba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9a0b60",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c07e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(TL_EfficientNetB0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55ead1",
   "metadata": {},
   "source": [
    "Le modèle EfficientNetB0 classe toutes les images comme si elles représentaient des champignons comestibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578760e",
   "metadata": {},
   "source": [
    "#### VGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(TL_VGG19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e88510",
   "metadata": {},
   "source": [
    "Le modèle VGG19 a tendance à classer toutes les images comme étant des images de champignons comestibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b9fe7",
   "metadata": {},
   "source": [
    "#### Xception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8700532",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(TL_Xception)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fc6fe",
   "metadata": {},
   "source": [
    "Le modèle Xception a tendance à classer les images comme étant des champignons non-comestibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab23080",
   "metadata": {},
   "source": [
    "#### ResNet50V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(TL_ResNet50V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaf8bd",
   "metadata": {},
   "source": [
    "Le modèle ResNetV2 est beaucoup plus nuancé que les autres, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3550174",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c15ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(TL_InceptionV3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4861158",
   "metadata": {},
   "source": [
    "Le modéle InceptionV3 fonctionne bien sur les champignons non-comestibles mais classe presque toute les images de champignons comestibles comme non-comestibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309199dd",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Les modèles testés fonctionnent plus ou moins pour l'une ou l'autre des catégories. Seul le modèle ResNet50V2 présente des résultats plus nuancés, ce qui ne signifie pas qu'il sera plus facile de l'améliorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b839e89",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Différents modèles ont été entrainés permettant de dresser un panorama et sélectionner un ou deux candidats pour poursuivre la mise au point d'un modele de classification de la comestibilité des champignons d'après une image.\n",
    "- Le modèle EfficienNetB0 ne se focalise pas sur les champignons et ses résultats (matrice de confusion) sont mauvais, il peut être écarté.\n",
    "- Le modèle VGG est écrate é car les résultats obtenus ne sont pas satisfaisant et le temps d'entrainement et beaucoup plus long que celui d'autres modèles.\n",
    "- Les modèles Xception et InceptionV3 permettent d'obtenir des résultats similaires avec un temps d'entrainement beaucoup plus court pour le modèle InceptionV3. Le modèle Xception ne sera pas étudié plus en profondeur.\n",
    "- Le modèle ResNet50V2 obtient des résultat assez différents des autres et se focalise bien sur les champignons durant l'apprentissage. Il sera également conservé pour la suite.\n",
    "\n",
    "Deux modèles seront donc étudiés plus en détail dans d'autres notebook : ResNet50V2 et InceptionV3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
