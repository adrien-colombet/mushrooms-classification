{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfd35e6",
   "metadata": {},
   "source": [
    "# Classifieur binaire de champignons avec Xception (comestible / non comestible) : tuning du learning_rate et de l'optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e8d53",
   "metadata": {},
   "source": [
    "## dans ce notebook :\n",
    "- entrainement d'un modèle avec un modèle Xception de base\n",
    "- optimisation du modèle Xception (choix du learning_rate et de l'optimizer\n",
    "\n",
    "Les inputs de ce notebook sont :\n",
    "- un dossier d'images contenant l'ensemble des images disponibles\n",
    "- un dataframe ou un fichier .csv contenant 2 colonnes :\n",
    "    - la première 'filepath' avec les noms des fichiers images,\n",
    "    - la seconde 'label' avec le nom des labels à identifier (0 = inedible, 1 = edible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, InputLayer, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c49a48",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des données d'entrée\n",
    "train_df = pd.read_csv(r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\reduced_dataset\\df_train.csv')\n",
    "val_df = pd.read_csv(r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\reduced_dataset\\df_test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des distributions des labels dans les différents jeux de données\n",
    "dataframes = ('train_df', 'val_df')\n",
    "counts = {\n",
    "    'inedible' : np.array([train_df.label.value_counts()[1], val_df.label.value_counts()[1]]),\n",
    "    'edible' : np.array([train_df.label.value_counts()[0], val_df.label.value_counts()[0]])\n",
    "}\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(2)\n",
    "for boolean, counts in counts.items():\n",
    "    p = ax.bar(dataframes, counts, width, label=boolean, bottom=bottom)\n",
    "    bottom += counts\n",
    "    \n",
    "ax.set_title('Labels distribution for differents balanced datasets')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de quelques paramètres\n",
    "IMAGE_DIR = r\"C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\reduced_dataset\\full\"\n",
    "W, H = 299, 299\n",
    "batch_size = 32\n",
    "SEED = 3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data generator pour le dataset d'entrainement\n",
    "train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 0.1,\n",
    "        height_shift_range = 0.1,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True\n",
    "        )\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].apply(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                    IMAGE_DIR,\n",
    "                                                    x_col=\"filename\",\n",
    "                                                    y_col=\"label\",\n",
    "                                                    target_size=(W,H),\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=SEED)\n",
    "\n",
    "# Création d'un data generator pour le dataset de validation\n",
    "val_datagen = ImageDataGenerator(\n",
    "       preprocessing_function = preprocess_input)\n",
    "\n",
    "val_df[\"label\"] = val_df[\"label\"].apply(str)\n",
    "val_generator = val_datagen.flow_from_dataframe(val_df,\n",
    "                                                IMAGE_DIR,\n",
    "                                                x_col=\"filename\",\n",
    "                                                y_col=\"label\",\n",
    "                                                target_size=(W,H),\n",
    "                                                class_mode=\"categorical\",\n",
    "                                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ec544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation du fonctionnement du data generator sur quelques images du dataset d'entrainement\n",
    "ex_df = train_df.sample(n=15).reset_index(drop=True)\n",
    "ex_gen = train_datagen.flow_from_dataframe(ex_df,IMAGE_DIR,x_col=\"filename\", y_col=\"label\",\n",
    "                                           target_size=(W,H), class_mode=\"categorical\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    for x, y in ex_gen:\n",
    "        im = x[0]\n",
    "        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores1(model)  :\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('loss by epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(model.history['accuracy'])\n",
    "    plt.plot(model.history['val_accuracy'])\n",
    "    plt.title('accuracy by epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14919485",
   "metadata": {},
   "source": [
    "## Xception de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f68248",
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = Xception(weights='imagenet', include_top=False)\n",
    "xception.trainable = False\n",
    "for layer in xception.layers: \n",
    "    layer.trainable = False\n",
    "model = Sequential()\n",
    "model.add(xception) # Ajout du modèle VGG16\n",
    "model.add(GlobalAveragePooling2D()) \n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323093d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche de possibilité d'utilisation d'une GPU\n",
    "print('GPU Available : ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle xception de base\n",
    "history_xception = model.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f562307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des metriques d'apprentissage\n",
    "plot_scores1(history_xception)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20230e7b",
   "metadata": {},
   "source": [
    "## Optimisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab67820",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de quelques learning rate à évaluer\n",
    "lr0 = 0.1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "lr3 = 0.0001\n",
    "lr4 = 0.00001\n",
    "lr5 = 0.000001\n",
    "\n",
    "# nombre d'epochs pour l'évaluation\n",
    "eval_epochs = 10\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr0, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr0 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr1, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr1 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr2, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr2 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr3, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr3 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr4, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr4 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )\n",
    "\n",
    "# définition de l'optimizer\n",
    "sgd = optimizers.SGD(learning_rate = lr5, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_lr5 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage graphique des résultats\n",
    "\n",
    "width = 0.5\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(221)\n",
    "plt.plot(history_lr0.history['loss'], label = 'lr = 0.1')\n",
    "plt.plot(history_lr1.history['loss'], label = 'lr = 0.01')\n",
    "plt.plot(history_lr2.history['loss'], label = 'lr = 0.001')\n",
    "plt.plot(history_lr3.history['loss'], label = 'lr = 0.0001')\n",
    "plt.plot(history_lr4.history['loss'], label = 'lr = 0.00001')\n",
    "plt.plot(history_lr5.history['loss'], label = 'lr = 0.000001')\n",
    "plt.title('loss by epoch for training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['lr = 0.1', 'lr = 0.01', 'lr = 0.001', 'lr = 0.0001', 'lr = 0.00001', 'lr = 0.000001'], loc='upper right')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(history_lr0.history['val_loss'], label = 'lr = 0.1')\n",
    "plt.plot(history_lr1.history['val_loss'], label = 'lr = 0.01')\n",
    "plt.plot(history_lr2.history['val_loss'], label = 'lr = 0.001')\n",
    "plt.plot(history_lr3.history['val_loss'], label = 'lr = 0.0001')\n",
    "plt.plot(history_lr4.history['val_loss'], label = 'lr = 0.00001')\n",
    "plt.plot(history_lr5.history['val_loss'], label = 'lr = 0.000001')\n",
    "plt.title('loss by epoch for validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['lr = 0.1', 'lr = 0.01', 'lr = 0.001', 'lr = 0.0001', 'lr = 0.00001', 'lr = 0.000001'], loc='lower right')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(history_lr0.history['accuracy'], label = 'lr = 0.1')\n",
    "plt.plot(history_lr1.history['accuracy'], label = 'lr = 0.01')\n",
    "plt.plot(history_lr2.history['accuracy'], label = 'lr = 0.001')\n",
    "plt.plot(history_lr3.history['accuracy'], label = 'lr = 0.0001')\n",
    "plt.plot(history_lr4.history['accuracy'], label = 'lr = 0.00001')\n",
    "plt.plot(history_lr5.history['accuracy'], label = 'lr = 0.000001')\n",
    "plt.title('accuracy by epoch for training')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['lr = 0.1', 'lr = 0.01', 'lr = 0.001', 'lr = 0.0001', 'lr = 0.00001', 'lr = 0.000001'], loc='lower right')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(history_lr0.history['val_accuracy'], label = 'lr = 0.1')\n",
    "plt.plot(history_lr1.history['val_accuracy'], label = 'lr = 0.01')\n",
    "plt.plot(history_lr2.history['val_accuracy'], label = 'lr = 0.001')   \n",
    "plt.plot(history_lr3.history['val_accuracy'], label = 'lr = 0.0001')     \n",
    "plt.plot(history_lr4.history['val_accuracy'], label = 'lr = 0.00001') \n",
    "plt.plot(history_lr5.history['val_accuracy'], label = 'lr = 0.000001') \n",
    "plt.title('accuracy by epoch for validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['lr = 0.1', 'lr = 0.01', 'lr = 0.001', 'lr = 0.0001', 'lr = 0.00001', 'lr = 0.000001'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221dec6",
   "metadata": {},
   "source": [
    "### Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de quelques learning rate à évaluer\n",
    "opt0 = optimizers.SGD()\n",
    "opt1 = optimizers.Adadelta()\n",
    "opt2 = optimizers.Adagrad()\n",
    "opt3 = optimizers.Adamax()\n",
    "opt4 = optimizers.Ftrl()\n",
    "opt5 = optimizers.Nadam()\n",
    "opt6 = optimizers.RMSprop()\n",
    "\n",
    "# nombre d'epochs pour l'évaluation\n",
    "eval_epochs = 10\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt0, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt0 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt1, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt1 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt2, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt2 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt3, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt3 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt4, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt4 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt5, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt5 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)\n",
    "\n",
    "# compilation du modèle\n",
    "model.compile(optimizer=opt6, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# entrainement du modèle xception de base\n",
    "history_opt6 = model.fit_generator(train_generator,\n",
    "                                                  epochs=eval_epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96675e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage graphique des résultats sur le choix de l'optimizer\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(221)\n",
    "plt.plot(history_opt0.history['loss'], label = 'SGD')\n",
    "plt.plot(history_opt1.history['loss'], label = 'Adadelta')\n",
    "plt.plot(history_opt2.history['loss'], label = 'Adagrad')\n",
    "plt.plot(history_opt3.history['loss'], label = 'Adamax')\n",
    "plt.plot(history_opt4.history['loss'], label = 'Ftrl')\n",
    "plt.plot(history_opt5.history['loss'], label = 'Nadam')\n",
    "plt.plot(history_opt6.history['loss'], label = 'RMSprop')\n",
    "plt.title('loss by epoch for training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['SGD', 'Adadelta', 'Adagrad', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop'], loc='lower right')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(history_opt0.history['val_loss'], label = 'SGD')\n",
    "plt.plot(history_opt1.history['val_loss'], label = 'Adadelta')\n",
    "plt.plot(history_opt2.history['val_loss'], label = 'Adagrad')\n",
    "plt.plot(history_opt3.history['val_loss'], label = 'Adamax')\n",
    "plt.plot(history_opt4.history['val_loss'], label = 'Ftrl')\n",
    "plt.plot(history_opt5.history['val_loss'], label = 'Nadam')\n",
    "plt.plot(history_opt6.history['val_loss'], label = 'RMSprop')\n",
    "plt.title('loss by epoch for validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['SGD', 'Adadelta', 'Adagrad', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop'], loc='lower right')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(history_opt0.history['accuracy'], label = 'SGD')\n",
    "plt.plot(history_opt1.history['accuracy'], label = 'Adadelta')\n",
    "plt.plot(history_opt2.history['accuracy'], label = 'Adagrad')\n",
    "plt.plot(history_opt3.history['accuracy'], label = 'Adamax')\n",
    "plt.plot(history_opt4.history['accuracy'], label = 'Ftrl')\n",
    "plt.plot(history_opt5.history['accuracy'], label = 'Nadam')\n",
    "plt.plot(history_opt6.history['accuracy'], label = 'RMSprop')\n",
    "plt.title('accuracy by epoch for training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['SGD', 'Adadelta', 'Adagrad', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop'], loc='lower right')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(history_opt0.history['val_accuracy'], label = 'SGD')\n",
    "plt.plot(history_opt1.history['val_accuracy'], label = 'Adadelta')\n",
    "plt.plot(history_opt2.history['val_accuracy'], label = 'Adagrad')\n",
    "plt.plot(history_opt3.history['val_accuracy'], label = 'Adamax')\n",
    "plt.plot(history_opt4.history['val_accuracy'], label = 'Ftrl')\n",
    "plt.plot(history_opt5.history['val_accuracy'], label = 'Nadam')\n",
    "plt.plot(history_opt6.history['val_accuracy'], label = 'RMSprop')\n",
    "plt.title('accuracy by epoch for validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['SGD', 'Adadelta', 'Adagrad', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop'], loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
