{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f17ab9",
   "metadata": {},
   "source": [
    "# Classification binaire par apprentissage non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca25427",
   "metadata": {},
   "source": [
    "L'objectif de ce notebook est de tenter d'établir une classification de la comestibilité d'un champignon à partir d'une image par la biais de méthodes non supervisées.\n",
    "Les inputs de ce notebook sont :\n",
    "- un fichier .csv contenant le nom des fichiers images et la cible correspondante\n",
    "- un dossier d'images.\n",
    "\n",
    "Ce notebook est inspiré de la page 'https://www.kaggle.com/code/hosen42/pneumonia-detection-using-traditional-ml-image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from yellowbrick.model_selection import learning_curve\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.utils import shuffle as shf\n",
    "import pickle\n",
    "import os\n",
    "import glob as gb\n",
    "import shutil\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "import warnings as wr\n",
    "wr.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90014a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import du jeu de données\n",
    "images_folder = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\dataset_cleaned'\n",
    "df = pd.read_csv(r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\df_XS.csv')\n",
    "validation_folder = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\val_dataset'\n",
    "validation_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\validation_dataset_wildfooduk_cleaned.csv'\n",
    "display(df.head(), df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ac5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dossier de sauvegarde des résultats\n",
    "parent_directory = os.path.dirname(images_folder)\n",
    "if not os.path.exists(os.path.join(parent_directory, 'Models_results')):\n",
    "    os.mkdir(os.path.join(parent_directory, 'Models_results'))\n",
    "\n",
    "models_results = os.path.join(parent_directory, 'Models_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd6a73",
   "metadata": {},
   "source": [
    "### préparation des images pour l'étude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93623107",
   "metadata": {},
   "source": [
    "#### séparation du jeu de données en 2 parties : train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42bd5e",
   "metadata": {},
   "source": [
    "L'objectif est de créer 2 partitions du dataframe original :\n",
    "- Train : partition qui servira à l'entrainement du modèle de classification,\n",
    "- Test : partition qui servira de set de validation lors de l'entraienemt du modèle,\n",
    "- Validation : Partition qui ne verra jamais l'entrainement permettant de comparer différents modèles entre eux.\n",
    "\n",
    "Le dataset complet est découpé en deux pour obtenir 'Train' et 'Test'.\n",
    "Le dataset de validation est le dataset WillFoodUK.\n",
    "\n",
    "Au final, depuis l'ensemble des données disponibles les datasets représentent :\n",
    "Test (df_test) : 20%\n",
    "Validation (df_val) : (3122 images)\n",
    "Train (df_train) : 80%\n",
    "\n",
    "Le random test de cette étude est 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation de la partition 'test' qui servira a tester le modele une fois celui-ci mis au point\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des noms d'images des dataframes selon la colonne cible df['edible']\n",
    "# Sélection les lignes pour edible = 1 et edible = 0\n",
    "df_edible_train = df_train.loc[df_train[\"edible\"] == 1]\n",
    "df_inedible_train = df_train.loc[df_train[\"edible\"] == 0]\n",
    "df_edible_test = df_test.loc[df_test[\"edible\"] == 1]\n",
    "df_inedible_test = df_test.loc[df_test[\"edible\"] == 0]\n",
    "\n",
    "# Extraction des noms des images dans des listes\n",
    "images_names_edible_train = df_edible_train[\"filename\"].values\n",
    "images_names_inedible_train = df_inedible_train[\"filename\"].values\n",
    "images_names_edible_train = list(images_names_edible_train)\n",
    "images_names_inedible_train = list(images_names_inedible_train)\n",
    "\n",
    "images_names_edible_test = df_edible_test[\"filename\"].values\n",
    "images_names_inedible_test = df_inedible_test[\"filename\"].values\n",
    "images_names_edible_test = list(images_names_edible_test)\n",
    "images_names_inedible_test = list(images_names_inedible_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8475c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de l'arborescence des fichiers\n",
    "train = 'train'\n",
    "test = 'test'\n",
    "\n",
    "edible = 'edible'\n",
    "inedible = 'inedible'\n",
    "XS_dataset = 'XS_dataset'\n",
    "parent_dir = os.path.dirname(images_folder)\n",
    "\n",
    "os.mkdir(os.path.join(parent_dir, XS_dataset))\n",
    "path_source = os.path.join(parent_dir, XS_dataset)\n",
    "\n",
    "os.mkdir(os.path.join(path_source, train))\n",
    "os.mkdir(os.path.join(path_source, test))\n",
    "\n",
    "path_train = os.path.join(path_source, train)\n",
    "path_test = os.path.join(path_source, test)\n",
    "\n",
    "\n",
    "os.mkdir(os.path.join(path_train, edible))\n",
    "os.mkdir(os.path.join(path_train, inedible))\n",
    "path_train_edible = os.path.join(path_train, edible)\n",
    "path_train_inedible = os.path.join(path_train, inedible)\n",
    "\n",
    "os.mkdir(os.path.join(path_test, edible))\n",
    "os.mkdir(os.path.join(path_test, inedible))\n",
    "path_test_edible = os.path.join(path_test, edible)\n",
    "path_test_inedible = os.path.join(path_test, inedible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# déclaration des chemins d'accés\n",
    "train = 'train'\n",
    "test = 'test'\n",
    "\n",
    "edible = 'edible'\n",
    "inedible = 'inedible'\n",
    "XS_dataset = 'XS_dataset'\n",
    "parent_dir = os.path.dirname(images_folder)\n",
    "\n",
    "path_source = os.path.join(parent_dir, XS_dataset)\n",
    "path_train = os.path.join(path_source, train)\n",
    "path_test = os.path.join(path_source, test)\n",
    "\n",
    "path_train_edible = os.path.join(path_train, edible)\n",
    "path_train_inedible = os.path.join(path_train, inedible)\n",
    "path_test_edible = os.path.join(path_test, edible)\n",
    "path_test_inedible = os.path.join(path_test, inedible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copie des images dans l'arborescence créée\n",
    "for image_name in images_names_edible_train :    \n",
    "    original_image_path = os.path.join(images_folder, image_name)\n",
    "    new_image_path = os.path.join(path_train_edible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "        \n",
    "for image_name in images_names_inedible_train :    \n",
    "    original_image_path = os.path.join(images_folder, image_name)\n",
    "    new_image_path = os.path.join(path_train_inedible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "        \n",
    "for image_name in images_names_edible_test :    \n",
    "    original_image_path = os.path.join(images_folder, image_name)\n",
    "    new_image_path = os.path.join(path_test_edible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "        \n",
    "for image_name in images_names_inedible_test :    \n",
    "    original_image_path = os.path.join(images_folder, image_name)\n",
    "    new_image_path = os.path.join(path_test_inedible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57595ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour le dataset de validation\n",
    "validation_df = pd.read_csv(validation_dataset)\n",
    "validation_df.drop(['kingdom', 'family', 'phylum', 'order', 'classes', 'genus', 'species'], axis = 1, inplace = True)\n",
    "\n",
    "# extraction des noms d'images des dataframes selon la colonne cible df['edible']\n",
    "# Sélection les lignes pour edible = 1 et edible = 0\n",
    "df_edible_val = validation_df.loc[validation_df[\"edible\"] == 1]\n",
    "df_inedible_val = validation_df.loc[validation_df[\"edible\"] == 0]\n",
    "# Extraction des noms des images dans des listes\n",
    "images_names_edible_val = df_edible_val[\"filename\"].values\n",
    "images_names_inedible_val = df_inedible_val[\"filename\"].values\n",
    "images_names_edible_val = list(images_names_edible_val)\n",
    "images_names_inedible_val = list(images_names_inedible_val)\n",
    "\n",
    "# création de l'arborescence des fichiers\n",
    "val = 'val'\n",
    "os.mkdir(os.path.join(path_source, val))\n",
    "path_val = os.path.join(path_source, val)\n",
    "os.mkdir(os.path.join(path_val, edible))\n",
    "os.mkdir(os.path.join(path_val, inedible))\n",
    "path_val_edible = os.path.join(path_val, edible)\n",
    "path_val_inedible = os.path.join(path_val, inedible)\n",
    "\n",
    "# copie des images dans l'arborescence créée\n",
    "for image_name in images_names_edible_val :    \n",
    "    original_image_path = os.path.join(validation_folder, image_name)\n",
    "    new_image_path = os.path.join(path_val_edible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "        \n",
    "for image_name in images_names_inedible_val :    \n",
    "    original_image_path = os.path.join(validation_folder, image_name)\n",
    "    new_image_path = os.path.join(path_val_inedible, image_name)\n",
    "    if os.path.exists(original_image_path):\n",
    "        shutil.copy(original_image_path, new_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db63a4",
   "metadata": {},
   "source": [
    "## Equilibrage des datasets train et test par suppression aléatoire d'images dans les dossiers sources de l'étude "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fcc49",
   "metadata": {},
   "source": [
    "Les 4 cellules ci-dessous sont à executer si besoin pour réaliser un sous échantillonnnage aléatoire des données images dans le but d'équilbrer les classes. Pour le dataset XS, cette partie n'est pas utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6afd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'une fonction permettant de supprimer n fichiers\n",
    "def delete_files(folder, n):\n",
    "    files = os.listdir(folder)\n",
    "    ensemble = set(files)\n",
    "    files_to_delete = random.sample(ensemble, n)\n",
    "    for file in files_to_delete:\n",
    "        os.remove(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b194583",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_edible = len(os.listdir(path_train_edible))\n",
    "count_train_inedible = len(os.listdir(path_train_inedible))\n",
    "count_test_edible = len(os.listdir(path_test_edible))\n",
    "count_test_inedible = len(os.listdir(path_test_inedible))\n",
    "\n",
    "n_train = count_train_inedible - count_train_edible\n",
    "n_test = count_test_inedible - count_test_edible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_files(path_train_inedible, n_train)\n",
    "delete_files(path_test_inedible, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_edible = len(os.listdir(path_train_edible))\n",
    "count_train_inedible = len(os.listdir(path_train_inedible))\n",
    "count_test_edible = len(os.listdir(path_test_edible))\n",
    "count_test_inedible = len(os.listdir(path_test_inedible))\n",
    "\n",
    "print('train_dataset :', count_train_edible, count_train_inedible)\n",
    "print('test_dataset :', count_test_edible, count_test_inedible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935ff0e",
   "metadata": {},
   "source": [
    "## Import du dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour le dataset d'entrainement\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "W, H = 128, 128\n",
    "\n",
    "# pour la partie train_edible\n",
    "# conversion des images sous la forme de matrices [W, H, 3]\n",
    "for image_path in os.listdir(path_train_edible):\n",
    "    image = Image.open(os.path.join(path_train_edible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    # Redimentionnement des images\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    # Ajout de l'image et de son dossier d'appartenance aux listes\n",
    "    X_train.append(image_resized)\n",
    "    y_train.append(1)\n",
    "\n",
    "# pour la partie train_inedible\n",
    "for image_path in os.listdir(path_train_inedible):\n",
    "    image = Image.open(os.path.join(path_train_inedible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    X_train.append(image_resized)\n",
    "    y_train.append(0)\n",
    "np.save('X_train',X_train)\n",
    "np.save('y_train',y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# application des mêmes étapes pour le dataset de test\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "W, H = 128,128\n",
    "\n",
    "# conversion des images sous la forme de matrices [W, H, 3]\n",
    "for image_path in os.listdir(path_test_edible):\n",
    "    image = Image.open(os.path.join(path_test_edible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    \n",
    "    X_test.append(image_resized)\n",
    "    y_test.append(1)\n",
    "\n",
    "\n",
    "for image_path in os.listdir(path_test_inedible):\n",
    "    image = Image.open(os.path.join(path_test_inedible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    X_test.append(image_resized)\n",
    "    y_test.append(0)\n",
    "np.save('X_test',X_test)\n",
    "np.save('y_test',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# application des mêmes étapes pour le dataset de validation\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "W, H = 128,128\n",
    "\n",
    "for image_path in os.listdir(path_val_edible):\n",
    "    image = Image.open(os.path.join(path_val_edible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    X_val.append(image_resized)\n",
    "    y_val.append(1)\n",
    "\n",
    "for image_path in os.listdir(path_val_inedible):\n",
    "    image = Image.open(os.path.join(path_val_inedible, image_path))\n",
    "    image_array = np.asarray(image)\n",
    "    image_resized = image_array[\n",
    "        :W, :H]\n",
    "    X_val.append(image_resized)\n",
    "    y_val.append(0)\n",
    "np.save('X_val',X_val)\n",
    "np.save('y_val',y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346773a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Image for training:\",(len(X_train)+len(X_test)))\n",
    "print(\"Total Image for validation:\",(len(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1985210",
   "metadata": {},
   "source": [
    "### Chargement des fichiers .npy comme arrays numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_testcontiennent les images sous forme d'arrays numpy\n",
    "# y_train, y_test contiennent les catégories de chaque image \n",
    "# chargement des fichiers .npy comme arrays numpy\n",
    "loaded_X_train = np.load('./X_train.npy')\n",
    "loaded_X_test = np.load('./X_test.npy')\n",
    "loaded_y_train = np.load('./y_train.npy')\n",
    "loaded_y_test = np.load('./y_test.npy')\n",
    "loaded_X_val = np.load('./X_val.npy')\n",
    "loaded_y_val = np.load('./y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43921dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_X_train.shape)\n",
    "#La dimension de X_train est de 20698 images de format 128x128 sur trois canaux de couleurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_X_test.shape)\n",
    "#La dimension de X_test est de 5120 images de format 128x128 sur trois canaux de couleurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2675c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_X_val.shape)\n",
    "#La dimension de X_val est de 3052 images de format 128x128 sur trois canaux de couleurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train et y_test contiennent les catégories de chaque image, avec 0 'inedible' ou 1 'edible'\n",
    "print(loaded_y_train.shape)\n",
    "print(loaded_y_test.shape)\n",
    "print(loaded_y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c682bf",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'inedible':0 ,'edible':1}\n",
    "# création d'une fonction permettant de retourner la catégorie (inedible / edible - 0 / 1) en fonction de la valeur de la cible\n",
    "def getcode(n) : \n",
    "    for x , y in code.items() : \n",
    "        if n == y : \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f16f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage de quelques images de champignons comestibles et non-comestibles du dataset d'entrainement\n",
    "plt.figure(figsize=(20,10))\n",
    "for n , i in enumerate(np.random.randint(0,len(loaded_X_train),16)): \n",
    "    plt.subplot(2,8,n+1)\n",
    "    plt.imshow(loaded_X_train[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(getcode(loaded_y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80816928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage d'un countplot permettant de visualiser le nombre d'images par catégorie disponible pour l'entrainement\n",
    "df_train = pd.DataFrame()\n",
    "df_train[\"labels\"]= loaded_y_train\n",
    "lab = df_train['labels']\n",
    "dist = lab.value_counts()\n",
    "sns.countplot(df_train, x = 'labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c436496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage de quelques images de champignons comestibles et non-comestibles du dataset de test\n",
    "plt.figure(figsize=(20,10))\n",
    "for n , i in enumerate(np.random.randint(0,len(loaded_X_test),16)): \n",
    "    plt.subplot(2,8,n+1)\n",
    "    plt.imshow(loaded_X_test[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(getcode(loaded_y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage d'un countplot permettant de visualiser le nombre d'images par catégorie disponible pour le test\n",
    "#As we can see inedible are over represented in all the data set. We will deal with such imbalance late\n",
    "df_test = pd.DataFrame()\n",
    "df_test[\"labels\"]= loaded_y_test\n",
    "lab = df_test['labels']\n",
    "dist = lab.value_counts()\n",
    "#play with pallette colors\n",
    "sns.countplot(df_test, x ='labels')\n",
    "plt.show()\n",
    "\n",
    "#Le dataset est bien équilibré entre les deux catégories dans les datasets d'entrainement et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6d770",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation d'une fonction permettant de visualiser l'intensité des pixels sur les canaux de couleur dans une image \n",
    "def plotHistogram(a):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(a)\n",
    "    histo = plt.subplot(1,2,2)\n",
    "    histo.set_ylabel('Count')\n",
    "    histo.set_xlabel('Pixel Intensity')\n",
    "    n_bins = 30\n",
    "    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.9)\n",
    "    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.9)\n",
    "    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistogram(loaded_X_train[np.random.randint(len(loaded_X_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d79049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistogram(loaded_X_test[np.random.randint(len(loaded_X_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d9413",
   "metadata": {},
   "source": [
    "### Mise à plat des images et mélange des datasets train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applatissement des images sous forme d'array sà 2 dimensions pour les datasets d'entrainement et de test\n",
    "X_train = loaded_X_train.reshape([-1, np.product((128,128,3))])\n",
    "X_test = loaded_X_test.reshape([-1, np.product((128,128,3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e79343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = loaded_y_train\n",
    "y_test = loaded_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mélange aléatoire des lignes des datasets \n",
    "X_train, y_train = shf(X_train, y_train, random_state=15)\n",
    "X_test, y_test = shf(X_test, y_test, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55e09b",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a18636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des images \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réalisation d'une ACP afin de réduire le nombre de features dans les datasets\n",
    "# # nous choisisons de conserver 95% de la variance représentée\n",
    "# #PCA    \n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358715ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats de la PCA\n",
    "path_save_name = os.path.join(models_results, 'pca_XS_dataset_overbalanced.joblib')\n",
    "dump(pca, path_save_name, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de features permettant de représenter 95% de la variance d'aprés l'ACP\n",
    "print('Number of components after PCA: ' + str(pca.n_components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76dd6da",
   "metadata": {},
   "source": [
    "### Préparation du dataset de validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applatissement des images pour le dataset de validation\n",
    "X_val = loaded_X_val.reshape([-1, np.product((128,128,3))])\n",
    "y_val = loaded_y_val\n",
    "\n",
    "\n",
    "print('X_val.shape before PCA', X_val.shape)\n",
    "print('y_val.shpae',y_val.shape)\n",
    "\n",
    "# Standardisation des images \n",
    "X_val = sc.fit_transform(X_val)\n",
    "\n",
    "# extraction des principales features obtenues par la PCA\n",
    "X_val = pca.transform(X_val)\n",
    "print('X_val.shape after PCA', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc20836",
   "metadata": {},
   "source": [
    "### équilibrage du dataset avec SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de60899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de SMOTE a X_train et y_train afin de générer de nouveaux individus et équilibrer les classes\n",
    "from collections import Counter\n",
    "smote = SMOTE(random_state = 11)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'impact de SMOTE sur le jeu de données d'entrainement\n",
    "print('Before SMOTE : ',Counter(y_train))\n",
    "print('After SMOTE : ',Counter(y_train_res))\n",
    "\n",
    "# Affichage des dimensions de X_train et y_train aprés SMOTE\n",
    "print('X_train shape :',X_train_res.shape)\n",
    "print('y_train shape :',y_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Création d'un LazyClassifier puis entrainement\n",
    "clf = LazyClassifier(verbose=0,\n",
    "                     ignore_warnings=True,\n",
    "                     custom_metric=None,\n",
    "                     predictions=True,\n",
    "                     random_state=12,\n",
    "                     classifiers='all')\n",
    "LazyClassifier()\n",
    "\n",
    "# A ajuster selon l'utilisation de SMOTE ou pas\n",
    "# model, predictions = clf.fit(X_train_res, x_test,y_train_res, y_test)\n",
    "model, predictions = clf.fit(X_train, X_test,y_train, y_test)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e594c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afifchage des 10 modèles présentant les meilleurs résultats sur la métrique 'Balanced Accuracy'\n",
    "top_10= model.sort_values(by='Balanced Accuracy', ascending=False).head(12)\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1f30a",
   "metadata": {},
   "source": [
    "### Visualisation des résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "line = px.line(data_frame= model ,y =[\"Accuracy\"] , markers = True)\n",
    "line.update_xaxes(title=\"Model\",\n",
    "              rangeslider_visible = False)\n",
    "line.update_yaxes(title = \"Accuracy\")\n",
    "line.update_traces(line_color=\"red\")\n",
    "line.update_layout(showlegend = True,\n",
    "    title = {\n",
    "        'text': 'Accuracy vs Model',\n",
    "        'y':0.94,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = px.line(data_frame= model ,y =[\"Accuracy\", \"ROC AUC\" , \"F1 Score\"] , markers = True)\n",
    "line.update_xaxes(title=\"Models\",\n",
    "               rangeslider_visible = False)\n",
    "line.update_yaxes(title = \"scores\")\n",
    "line.update_layout(showlegend = True,\n",
    "    title = {\n",
    "        'text': 'Accuracy, ROC AUC and F1 Score vs Models',\n",
    "        'y':0.94,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = px.line(data_frame= model ,y =[\"Time Taken\"] , markers = True)\n",
    "line.update_xaxes(title=\"Model\",\n",
    "              rangeslider_visible = False)\n",
    "line.update_yaxes(title = \"Time(s)\")\n",
    "line.update_traces(line_color=\"purple\")\n",
    "line.update_layout(showlegend = True,\n",
    "    title = {\n",
    "        'text': 'TIME TAKEN vs Model',\n",
    "        'y':0.94,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4094f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats de LazyPredict\n",
    "path_save_name = os.path.join(models_results, 'LazyPredict_XS_dataset_withoutSMOTE.joblib')\n",
    "dump(clf, path_save_name, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcdc2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, PrecisionRecallDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bcf9d",
   "metadata": {},
   "source": [
    "### GaussianNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9767223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB , GaussianNB, BernoulliNB\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb=gaussian_nb.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training Accuracy: \", gaussian_nb.score(X_train_res, y_train_res)*100) # Check training accuracy\n",
    "print (\"Validation Accuracy: \", gaussian_nb.score(X_test,y_test)*100) # Check validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_pred = gaussian_nb.predict(X_val)\n",
    "accuracy_score(gaussian_nb_pred,y_val)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val,gaussian_nb_pred)) #main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ba185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val,gaussian_nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal')\n",
    "plt.title(f'Confusion matrix for {extra_trees_classifier}')\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats de GaussianNB\n",
    "path_save_name = os.path.join(models_results, 'GaussienNB_XS_dataset_with_SMOTE.joblib')\n",
    "dump(gaussian_nb, path_save_name, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(gaussian_nb,X_test,y_test)\n",
    "plt.plot([1,0],[1,0],'go--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9c5cc",
   "metadata": {},
   "source": [
    "### BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "bagging_classifier = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=0)\n",
    "bagging_classifier = bagging_classifier.fit(X_train_res, y_train_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training Accuracy: \", bagging_classifier.score(X_train_res, y_train_res)*100) # Check training accuracy\n",
    "print (\"validation Accuracy: \", bagging_classifier.score(X_test,y_test)*100) # Check testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8eb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_classifier_pred = bagging_classifier.predict(X_val)\n",
    "accuracy_score(bagging_classifier_pred,y_val)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419df3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val,bagging_classifier_pred)) #main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_val,bagging_classifier_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff583506",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal')\n",
    "plt.title(f'Confusion matrix for {extra_trees_classifier}')\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea83edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats de BaggingClassifier\n",
    "path_save_name = os.path.join(models_results, 'BaggingClassifier_XS_dataset_with_SMOTE.joblib')\n",
    "dump(bagging_classifier, path_save_name, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(bagging_classifier,X_test,y_test)\n",
    "plt.plot([1,0],[1,0],'go--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674b11c",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a776e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_trees_classifier = ExtraTreesClassifier()\n",
    "extra_trees_classifier=extra_trees_classifier.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc40cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training Accuracy: \", extra_trees_classifier.score(X_train_res, y_train_res)*100) # Check training accuracy\n",
    "print (\"validation Accuracy: \", extra_trees_classifier.score(X_test,y_test)*100) # Check testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_classifier_pred = extra_trees_classifier.predict(X_val)\n",
    "accuracy_score(extra_trees_classifier_pred,y_val)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199805ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val,extra_trees_classifier_pred)) #main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_val,extra_trees_classifier_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal')\n",
    "plt.title(f'Confusion matrix for {extra_trees_classifier}')\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats de ExtraTreesClassifier\n",
    "path_save_name = os.path.join(models_results, 'ExtraTreesClassifier_XS_dataset_with_SMOTE.joblib')\n",
    "dump(extra_trees_classifier, path_save_name, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
