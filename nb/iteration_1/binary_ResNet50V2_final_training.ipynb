{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbdf9df",
   "metadata": {},
   "source": [
    "# Final_training ResNet50V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a67154",
   "metadata": {},
   "source": [
    "L'objectif de ce notebook est de réaliser l'entrainement final de ResNet50V2 avec les hyperparamètres identifiés sur un peu plus d'échocs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "\n",
    "import  keras\n",
    "import tensorflow as tf # Utilisation de tensorflow v2.9.1\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création des liens vers les dossiers et fichiers source\n",
    "images_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\FFD_images_dataset'\n",
    "train_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\train_FFDataframe_full_undersampling.csv'\n",
    "test_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\test_FFDataframe_full_undersampling.csv'\n",
    "validation_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\val_FFDataframe_full.csv'\n",
    "\n",
    "# dossier ou sauver les résultats obtenus sur les modèles\n",
    "save_models_results = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\Models_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de quelques fonctions utiles\n",
    "\n",
    "# affichage des metriques (accuracy, loss) d'entrainement d'un modèle\n",
    "def plot_scores(model, title):\n",
    "    '''\n",
    "    Arg :\n",
    "    model : model dont on souhaite afficher les metriques\n",
    "    Return:\n",
    "    plot des métriques Accuracy et loss sur les datasets train et test\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.rcParams['figure.figsize'] = [14,4]\n",
    "\n",
    "    # Créer la figure\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.gcf().subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0.3, hspace = 0.3)\n",
    "    # Créer les 4 graphiques\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # Tracer les données sur les graphiques\n",
    "    ax1.plot(model.history['accuracy'], label = \"train\")\n",
    "    ax1.plot(model.history['val_accuracy'], label = \"test\")\n",
    "    ax1.legend(loc = \"lower right\")\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')    \n",
    "\n",
    "    ax2.plot(model.history['loss'], label = \"train\")\n",
    "    ax2.plot(model.history['val_loss'], label = \"test\")\n",
    "    ax2.legend(loc = \"upper right\")\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')  \n",
    "    plt.title(title, loc = \"left\")\n",
    "    plt.show()\n",
    "    \n",
    "# affichage de la matrice de confusion du dataset de validation\n",
    "def show_confusion_matrix(model):\n",
    "    '''\n",
    "    Args :\n",
    "    model : modele à utiliser pour fair eles predictions\n",
    "   \n",
    "    Return :\n",
    "    plot de la matrice de confusion\n",
    "    '''\n",
    "    # réalisation des prédiction pour le modèle\n",
    "    model_pred=model.predict(val_generator, steps=val_steps, verbose=1)\n",
    "    y_pred = []\n",
    "    for element in model_pred:\n",
    "        pred = np.argmax(element)\n",
    "        y_pred.append(pred)\n",
    "    y_val = df_val.edible.to_list()\n",
    "    confusion_mtx = confusion_matrix(y_val, y_pred)\n",
    "    #\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mtx)\n",
    "    disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal', colorbar = False)\n",
    "    plt.title(f'Confusion matrix for {model}')\n",
    "    plt.ylabel('True label', fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.xlabel('Predicted label', fontsize = 20)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    \n",
    "# création d'une fonction permettant de compiler un modèle\n",
    "def compile_model(model, optimizer, loss, metrics):\n",
    "    '''\n",
    "    Args :\n",
    "    model : model à compiler\n",
    "    optimizer :  choix de l'optimizer à utiliser durant l'entrainement\n",
    "    loss : fonction de loss à utiliser durant l'entrainement sous la forme : \"loss\"\n",
    "    metrics : metrique à évaluer durant l'entrainement sou sla forme : [\"metrics\"]\n",
    "    '''\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des dataframes\n",
    "df_train = pd.read_csv(train_dataset)\n",
    "df_test = pd.read_csv(test_dataset)\n",
    "df_val = pd.read_csv(validation_dataset)\n",
    "\n",
    "# affichage de quelques infos sur ces dataframes + affichage d'une figure de répartition des catégories\n",
    "display(df_train.head(), df_test.info(), df_val.info())\n",
    "\n",
    "# génération des données du graph\n",
    "inedible = []\n",
    "edible = []\n",
    "\n",
    "dataframes = [df_train, df_test, df_val]\n",
    "for dataframe in dataframes:\n",
    "    count_inedible = dataframe['edible'].value_counts()[0]\n",
    "    inedible.append(count_inedible)\n",
    "    count_edible = dataframe['edible'].value_counts()[1]\n",
    "    edible.append(count_edible)   \n",
    "\n",
    "data = ['df_train', 'df_test', 'df_val']\n",
    "edibility = {'inedible': inedible, 'edible' : edible}\n",
    "\n",
    "colonnes = ['df_train', 'df_test', 'df_val']\n",
    "sex_counts = {\n",
    "    'inedible': inedible,\n",
    "    'edible': edible\n",
    "}\n",
    "\n",
    "width = 0.6\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "for i, j in edibility.items():\n",
    "    p = ax.bar(data, j, width, label=i, bottom=bottom)\n",
    "    bottom += j\n",
    "    ax.bar_label(p, label_type='center')\n",
    "ax.set_title('Number of images by category')\n",
    "ax.legend(title = 'categories')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f8894",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118205c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création par transfert learning d'un modèle de type ResNet50V2 à deux sorties\n",
    "TL_ResNet50V2 = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in TL_ResNet50V2.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(TL_ResNet50V2.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "TL_ResNet50V2 = Model(TL_ResNet50V2.input, output, name = 'TL_ResNet50V2')\n",
    "TL_ResNet50V2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c131a3b",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0127f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de quelques paramètres\n",
    "batch_size = 128\n",
    "SEED = 3\n",
    "epochs = 30\n",
    "W, H = 224, 224\n",
    "optimizer = optimizers.Adam(learning_rate = 0.001)\n",
    "loss_function = \"hinge\"\n",
    "metrics = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset d'entrainement\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_train[\"edible\"] = df_train[\"edible\"].apply(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(df_train, images_dataset,\n",
    "                                                    x_col=\"filename\",\n",
    "                                                    y_col=\"edible\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=SEED)\n",
    "\n",
    "# Création d'un DataGenerator pour le dataset de test\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_test[\"edible\"] = df_test[\"edible\"].apply(str)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(df_test, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  y_col=\"edible\",\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6468595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation du modèle\n",
    "compile_model(TL_ResNet50V2, optimizer, loss = loss_function, metrics = metrics)\n",
    "\n",
    "# création de callbacks\n",
    "checkpointer_ResNet50V2 = ModelCheckpoint(filepath=os.path.join(save_models_results, \"VF_ResNet50V2.hdf5\"),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "CSV_logger_ResNet50V2 = CSVLogger(filename = 'logger_VF_ResNet50V2.csv',\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "callbacks_ResNet50V2 = [checkpointer_ResNet50V2, CSV_logger_ResNet50V2]\n",
    "\n",
    "# entrainement du modèle\n",
    "start_time = time.time()\n",
    "history_VF_ResNet50V2 = TL_ResNet50V2.fit_generator(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_ResNet50V2)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb13cbe",
   "metadata": {},
   "source": [
    "## Affichage des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216aa999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des courbes d'entrainement\n",
    "plot_scores(history_VF_ResNet50V2, \"entrainement final de ResNet50V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset de test\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(df_test, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  class_mode=None,\n",
    "                                                  batch_size=1)\n",
    "test_steps = len(df_test)\n",
    "df_test[\"edible\"] = df_test[\"edible\"].apply(int)\n",
    "\n",
    "# Création d'un DataGenerator pour le dataset de validation\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(df_val, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  class_mode=None,\n",
    "                                                  batch_size=1)\n",
    "val_steps = len(df_val)\n",
    "df_val[\"edible\"] = df_val[\"edible\"].apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de la matrice de confusion sur le dataset de test\n",
    "\n",
    "test_steps = len(df_test)\n",
    "test_pred=TL_ResNet50V2.predict(test_generator, steps=test_steps, verbose=1)\n",
    "y_pred_test = []\n",
    "for element in test_pred:\n",
    "    pred = np.argmax(element)\n",
    "    y_pred_test.append(pred)\n",
    "y_test = df_test.edible.to_list()\n",
    "\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred_test)\n",
    "plt.rcParams['font.size'] = 20\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mtx)\n",
    "disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal', colorbar = False)\n",
    "plt.title('Confusion matrix for ResNet50v2 dataset de test')\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de la matrice de confusion sur le dataset de validation\n",
    "\n",
    "val_pred=TL_ResNet50V2.predict(val_generator, steps=val_steps, verbose=1)\n",
    "y_pred_val = []\n",
    "for element in val_pred:\n",
    "    pred = np.argmax(element)\n",
    "    y_pred_val.append(pred)\n",
    "y_val = df_val.edible.to_list()\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_val, y_pred_val)\n",
    "plt.rcParams['font.size'] = 20\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mtx)\n",
    "disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal', colorbar = False)\n",
    "plt.title('Confusion matrix for ResNet50v2 dataset de validation')\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'une liste des probabilité d echampignons comestibles du df_val\n",
    "pred_val_edible = []\n",
    "for element in val_pred:\n",
    "    pred = element[1]\n",
    "    pred_val_edible.append(pred)\n",
    "pred_val_edible = pd.Series(pred_val_edible)\n",
    "\n",
    "# affichage d'un histogramme de densité des probabilité d'avoir un champignon comestible\n",
    "fig = plt.figure(figsize =(5, 5))\n",
    "#plt.hist(pred_val_edible, bins = 20, density = True)\n",
    "\n",
    "ax = pred_val_edible.plot.density()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('probs')\n",
    "ax.set_ylabel('density') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ed7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des résultats du modele VF_ResNet50V2\n",
    "path_save_name = os.path.join(save_models_results, 'history_VF_ResNet50V2.joblib')\n",
    "dump(history_VF_ResNet50V2, path_save_name, 3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
