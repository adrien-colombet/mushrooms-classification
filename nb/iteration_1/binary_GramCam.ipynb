{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a270d7",
   "metadata": {},
   "source": [
    "# Binary classification of mushrooms with tensorflow : simple_model and GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f8aef",
   "metadata": {},
   "source": [
    "## dans ce notebook :\n",
    "- entrainement d'un modèle avec une architecture simple\n",
    "- interprétation des résultats avec des GradCAM\n",
    "\n",
    "Les inputs de ce notebook sont :\n",
    "- un dossier d'images contenant l'ensemble des images disponibles\n",
    "- un dataframe ou un fichier .csv contenant 2 colonnes :\n",
    "    - la première 'filepath' avec les noms des fichiers images,\n",
    "    - la seconde 'label' avec le nom des labels à identifier (0 = inedible, 1 = edible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5907318",
   "metadata": {},
   "source": [
    "###\n",
    "Ce notebook est inspiré du travail de HOA NGUYEN (https://www.kaggle.com/code/nguyenhoa/dog-cat-classifier-gradcam-with-tensorflow-2-0/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des librairies nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fonctionne avec tensorflow v2.9.1\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2229094",
   "metadata": {},
   "source": [
    "## Préparation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd52683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chermin du dossier source et paramètres de base\n",
    "IMAGE_DIR = r\"C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\Binary_Classification\\reduced_dataset\\input\\full_reduced_dataset\\train\"\n",
    "H = 224 # A adapter en fonction de l'input du modele à entrainer\n",
    "W = 224 # A adapter en fonction de l'input du modele à entrainer\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "SEED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da608fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import du fichier .csv ou création du dataframe\n",
    "df = pd.read_csv(r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\Binary_Classification\\reduced_dataset\\input\\full_reduced_dataset\\train_labels.csv')\n",
    "df.drop(['Unnamed: 0', 'kingdom', 'phylum', 'family', 'order', 'classes', 'genus', 'species'], axis = 1, inplace = True)\n",
    "df.rename({'image_lien' : 'filename', 'edible' : 'label'}, axis = 1, inplace = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7d57f",
   "metadata": {},
   "source": [
    "## Création des sets de données : train, validation, test\n",
    "Le dataset initial est séparé en 3 parties :\n",
    "- une partie test représentant 20% des données initiales (test_df)\n",
    "- une partie train représentant 64 % des données initiales (train_df)\n",
    "- une partie validation représentant 16% des données initiales (val_df)\n",
    "\n",
    "Train, servira à l'entrainement, Val à l'évaluation durant la phase d'entrainement, Test à comparer les résultats issus de différents modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation du dataset initial en 2 parties (train et test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state = SEED, stratify = df.label)\n",
    "train_df.sample(frac=1, random_state=SEED)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Séparation du dataset train en 2 parties (train et val)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state = SEED, stratify = train_df.label)\n",
    "train_df.sample(frac=1, random_state=SEED)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c58909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des différents dataframes\n",
    "output_dir = os.path.dirname(IMAGE_DIR)\n",
    "\n",
    "train_df.to_csv(f\"{output_dir}/train_df.csv\")\n",
    "val_df.to_csv(f\"{output_dir}/val_df.csv\")\n",
    "test_df.to_csv(f\"{output_dir}/test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73075a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des distributions des labels dans les différents jeux de données\n",
    "dataframes = ('train_df', 'val_df', 'test_df')\n",
    "counts = {\n",
    "    'inedible' : np.array([train_df.label.value_counts()[1] / len(train_df), val_df.label.value_counts()[1] / len(val_df), test_df.label.value_counts()[1] / len(test_df)]),\n",
    "    'edible' : np.array([train_df.label.value_counts()[0] / len(train_df), val_df.label.value_counts()[0] / len(val_df), test_df.label.value_counts()[0] / len(test_df)])\n",
    "}\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "for boolean, counts in counts.items():\n",
    "    p = ax.bar(dataframes, counts, width, label=boolean, bottom=bottom)\n",
    "    bottom += counts\n",
    "ax.set_title('Labels distribution for differents datasets')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70603b37",
   "metadata": {},
   "source": [
    "### Rééquilibrage du jeu de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc409234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour la partie train\n",
    "label_counts = train_df.groupby('label').size()\n",
    "train_df_X = train_df.groupby('label').apply(lambda x : x.sample(label_counts.min()))\n",
    "\n",
    "# pour la partie val\n",
    "label_counts = val_df.groupby('label').size()\n",
    "val_df_X = val_df.groupby('label').apply(lambda x : x.sample(label_counts.min()))\n",
    "\n",
    "# pour la partie test\n",
    "label_counts = test_df.groupby('label').size()\n",
    "test_df_X = test_df.groupby('label').apply(lambda x : x.sample(label_counts.min()))\n",
    "\n",
    "\n",
    "\n",
    "# sauvegarde des différents dataframes\n",
    "output_dir = os.path.dirname(IMAGE_DIR)\n",
    "\n",
    "train_df_X.to_csv(f\"{output_dir}/train_df_X.csv\")\n",
    "val_df_X.to_csv(f\"{output_dir}/val_df_X.csv\")\n",
    "test_df_X.to_csv(f\"{output_dir}/test_df_X.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Visualisation des distributions des labels dans les différents jeux de données rééquilibrés\n",
    "dataframes = ('train_df_X', 'val_df_X', 'test_df_X')\n",
    "counts = {\n",
    "    'inedible' : np.array([train_df_X.label.value_counts()[1] / len(train_df_X), val_df_X.label.value_counts()[1] / len(val_df_X), test_df_X.label.value_counts()[1] / len(test_df_X)]),\n",
    "    'edible' : np.array([train_df_X.label.value_counts()[0] / len(train_df_X), val_df_X.label.value_counts()[0] / len(val_df_X), test_df_X.label.value_counts()[0] / len(test_df_X)])\n",
    "}\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "for boolean, counts in counts.items():\n",
    "    p = ax.bar(dataframes, counts, width, label=boolean, bottom=bottom)\n",
    "    bottom += counts\n",
    "ax.set_title('Labels distribution for differents balanced datasets')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501c4b6",
   "metadata": {},
   "source": [
    "## Exploration de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "inedible = list(df[df.label==0].filename)\n",
    "edible = list(df[df.label==1].filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb65daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de fonctions permettant de générer une grille de 100 images avec des transformations (rotation) aléatoires\n",
    "# code adaptés avec quelques modifications de\n",
    "# https://www.kaggle.com/serkanpeldek/keras-cnn-transfer-learnings-on-cats-dogs-dataset\n",
    "\n",
    "def get_side(img, side_type, n = 5):\n",
    "    h, w, c = img.shape\n",
    "    if side_type == \"horizontal\":\n",
    "        return np.ones((h,n,c))\n",
    "    return np.ones((n,w,c))\n",
    "\n",
    "def show_gallery(im_ls,n=5, shuffle=True):\n",
    "    images = []\n",
    "    vertical_images = []\n",
    "    if shuffle:\n",
    "        random.shuffle(im_ls)\n",
    "    vertical_images = []\n",
    "    for i in range(n*n):\n",
    "        img = load_img(os.path.join(IMAGE_DIR,im_ls[i]), target_size=(W,H))\n",
    "        img = img_to_array(img)\n",
    "        hside = get_side(img,side_type=\"horizontal\")\n",
    "        images.append(img)\n",
    "        images.append(hside)\n",
    "        \n",
    "        if (i+1) % n == 0:\n",
    "            himage=np.hstack((images))\n",
    "            vside = get_side(himage, side_type=\"vertical\")\n",
    "            vertical_images.append(himage)\n",
    "            vertical_images.append(vside)\n",
    "            \n",
    "            images = []\n",
    "        \n",
    "    gallery = np.vstack((vertical_images))\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gallery.astype(np.uint8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35688d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation d'une grille d'images de champignons comestibles\n",
    "show_gallery(edible, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation d'une grille d'images de champignons non-comestibles\n",
    "show_gallery(inedible, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c96b8",
   "metadata": {},
   "source": [
    "## Définition des classes GradCAM & GuidedGradCAM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655a5a0",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    # Adapté avec quelques modifications de  https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
    "    def __init__(self, model, layerName=None):\n",
    "        \"\"\"\n",
    "        model: pre-softmax layer (logit layer)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "            \n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "    \n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM\")\n",
    "            \n",
    "    def compute_heatmap(self, image, classIdx, upsample_size, eps=1e-5):\n",
    "        gradModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output, self.model.output]\n",
    "        )\n",
    "        # record operations for automatic differentiation\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOuts, preds) = gradModel(inputs) # preds after softmax\n",
    "            loss = preds[:,classIdx]\n",
    "        \n",
    "        # compute gradients with automatic differentiation\n",
    "        grads = tape.gradient(loss, convOuts)\n",
    "        # discard batch\n",
    "        convOuts = convOuts[0]\n",
    "        grads = grads[0]\n",
    "        norm_grads = tf.divide(grads, tf.reduce_mean(tf.square(grads)) + tf.constant(eps))\n",
    "        \n",
    "        # compute weights\n",
    "        weights = tf.reduce_mean(norm_grads, axis=(0,1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOuts), axis=-1)\n",
    "        \n",
    "        # Apply reLU\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam/np.max(cam)\n",
    "        cam = cv2.resize(cam, upsample_size,interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # convert to 3D\n",
    "        cam3 = np.expand_dims(cam, axis=2)\n",
    "        cam3 = np.tile(cam3, [1,1,3])\n",
    "        \n",
    "        return cam3\n",
    "    \n",
    "def overlay_gradCAM(img, cam3):\n",
    "    cam3 = np.uint8(255*cam3)\n",
    "    cam3 = cv2.applyColorMap(cam3, cv2.COLORMAP_JET)\n",
    "    \n",
    "    new_img = 0.3*cam3 + 0.5*img\n",
    "    \n",
    "    return (new_img*255.0/new_img.max()).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4e5a4",
   "metadata": {},
   "source": [
    "### GuidedGradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4516980",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def guidedRelu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
    "    return tf.nn.relu(x), grad\n",
    "\n",
    "# Reference: https://github.com/eclique/keras-gradcam with adaption to tensorflow 2.0  \n",
    "class GuidedBackprop:\n",
    "    def __init__(self,model, layerName=None):\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "        self.gbModel = self.build_guided_model()\n",
    "        \n",
    "        if self.layerName == None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply Guided Backpropagation\")\n",
    "\n",
    "    def build_guided_model(self):\n",
    "        gbModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output]\n",
    "        )\n",
    "        layer_dict = [layer for layer in gbModel.layers[1:] if hasattr(layer,\"activation\")]\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == tf.keras.activations.relu:\n",
    "                layer.activation = guidedRelu\n",
    "        \n",
    "        return gbModel\n",
    "    \n",
    "    def guided_backprop(self, images, upsample_size):\n",
    "        \"\"\"Guided Backpropagation method for visualizing input saliency.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(images, tf.float32)\n",
    "            tape.watch(inputs)\n",
    "            outputs = self.gbModel(inputs)\n",
    "\n",
    "        grads = tape.gradient(outputs, inputs)[0]\n",
    "\n",
    "        saliency = cv2.resize(np.asarray(grads), upsample_size)\n",
    "\n",
    "        return saliency\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Same normalization as in:\n",
    "    # https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    \n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x = x.copy()\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed8973",
   "metadata": {},
   "source": [
    "### Création d'une fonction de visualisation des GradCAM et GuidedGradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gradCAMs(model, gradCAM, GuidedBP, im_ls, n=3, decode={}):\n",
    "    \"\"\"\n",
    "    model: softmax layer\n",
    "    \"\"\"\n",
    "    random.shuffle(im_ls)\n",
    "    plt.subplots(figsize=(30, 10*n))\n",
    "    k=1\n",
    "    for i in range(n):\n",
    "        img = cv2.imread(os.path.join(IMAGE_DIR,im_ls[i]))\n",
    "        upsample_size = (img.shape[1],img.shape[0])\n",
    "        if (i+1) == len(df):\n",
    "            break\n",
    "        # Show original image\n",
    "        plt.subplot(n,3,k)\n",
    "        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Filename: {}\".format(im_ls[i]), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        # Show overlayed grad\n",
    "        plt.subplot(n,3,k+1)\n",
    "        im = img_to_array(load_img(os.path.join(IMAGE_DIR,im_ls[i]), target_size=(W,H)))\n",
    "        x = np.expand_dims(im, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        preds = model.predict(x)\n",
    "        idx = preds.argmax()\n",
    "        if len(decode)==0:\n",
    "            res = decode_predictions(preds)[0][0][1:]\n",
    "        else:\n",
    "            res = [decode[idx],preds.max()]\n",
    "        cam3 = gradCAM.compute_heatmap(image=x, classIdx=idx, upsample_size=upsample_size)\n",
    "        new_img = overlay_gradCAM(img, cam3)\n",
    "        new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(new_img)\n",
    "        plt.title(\"GradCAM - Pred: {}. Prob: {}\".format(res[0],res[1]), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Show guided GradCAM\n",
    "        plt.subplot(n,3,k+2)\n",
    "        gb = GuidedBP.guided_backprop(x, upsample_size)\n",
    "        guided_gradcam = deprocess_image(gb*cam3)\n",
    "        guided_gradcam = cv2.cvtColor(guided_gradcam, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(guided_gradcam)\n",
    "        plt.title(\"Guided GradCAM\", fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        k += 3\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d1c4a",
   "metadata": {},
   "source": [
    "## Mise au point d'un modèle de classification binaire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591903fb",
   "metadata": {},
   "source": [
    "### Data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data generator pour le dataset d'entrainement\n",
    "train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range = 10,\n",
    "        width_shift_range = 0.1,\n",
    "        height_shift_range = 0.1,\n",
    "        zoom_range = 0.1,\n",
    "        horizontal_flip = True\n",
    "        )\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].apply(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df, IMAGE_DIR,x_col=\"filename\", y_col=\"label\",\n",
    "                                                    target_size=(W,H), class_mode=\"categorical\",\n",
    "                                                   batch_size=batch_size, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data generator pour le dataset d'entrainement rééquilibré\n",
    "train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range = 10,\n",
    "        width_shift_range = 0.1,\n",
    "        height_shift_range = 0.1,\n",
    "        zoom_range = 0.1,\n",
    "        horizontal_flip = True\n",
    "        )\n",
    "\n",
    "train_df_X[\"label\"] = train_df_X[\"label\"].apply(str)\n",
    "\n",
    "train_generator_X = train_datagen.flow_from_dataframe(train_df_X, IMAGE_DIR,x_col=\"filename\", y_col=\"label\",\n",
    "                                                    target_size=(W,H), class_mode=\"categorical\",\n",
    "                                                   batch_size=batch_size, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d490f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data generator pour le dataset de validation\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "val_df[\"label\"] = val_df[\"label\"].apply(str)\n",
    "val_generator = val_datagen.flow_from_dataframe(val_df, IMAGE_DIR, x_col=\"filename\", y_col=\"label\",\n",
    "                                               target_size=(W,H), class_mode=\"categorical\",\n",
    "                                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data generator pour le dataset de validation rééquilibré\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "val_df_X[\"label\"] = val_df_X[\"label\"].apply(str)\n",
    "val_generator_X = val_datagen.flow_from_dataframe(val_df_X, IMAGE_DIR, x_col=\"filename\", y_col=\"label\",\n",
    "                                               target_size=(W,H), class_mode=\"categorical\",\n",
    "                                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01004bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation du fonctionnement du data generator sur quelques images du dataset d'entrainement\n",
    "ex_df = train_df.sample(n=15).reset_index(drop=True)\n",
    "ex_gen = train_datagen.flow_from_dataframe(ex_df,IMAGE_DIR,x_col=\"filename\", y_col=\"label\",\n",
    "                                           target_size=(W,H), class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fee511",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    for x, y in ex_gen:\n",
    "        im = x[0]\n",
    "        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bdf93",
   "metadata": {},
   "source": [
    "### Modèle 1 : modèle simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un modèle simple\n",
    "simple_model = tf.keras.Sequential(name = 'simple_model')\n",
    "# block1\n",
    "simple_model.add(Conv2D(filters = 30,                    # Nombre de filtres\n",
    "                        kernel_size = (5, 5),            # Shape du kernel\n",
    "                        input_shape = (224, 224, 3),     # Shape de l'entrée\n",
    "                        activation = 'relu',             # Fonction d'activation\n",
    "                        name = 'conv1_block1_out'))      # nom de la couche\n",
    "simple_model.add(MaxPooling2D(pool_size = (2, 2),\n",
    "                              name = 'maxpo1_block1_out'))\n",
    "simple_model.add(Conv2D(filters = 16,                    \n",
    "                        kernel_size = (3, 3),\n",
    "                        activation = 'relu',\n",
    "                        name = 'conv2_block1_out'))\n",
    "simple_model.add(MaxPooling2D(pool_size = (2, 2),\n",
    "                              name = 'maxpo2_block1_out'))\n",
    "# block2\n",
    "simple_model.add(Flatten(name = 'flat1_block2_out'))\n",
    "simple_model.add(Dropout(rate = 0.2,\n",
    "                         name = 'drop1_block2_out'))\n",
    "#block3\n",
    "simple_model.add(Dense(units = 128,\n",
    "                       activation = 'relu',\n",
    "                       name = 'den1_block3_out'))\n",
    "simple_model.add(Dense(units = 2,\n",
    "                       activation = 'softmax',\n",
    "                       name = 'den2_block3_out'))\n",
    "\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841986ac",
   "metadata": {},
   "source": [
    "### Modéle 2 : transfert learning de EfficientNetB0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91afc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "efficientnet = EfficientNetB0(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in efficientnet.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(efficientnet.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "efficientnet = Model(efficientnet.input, output, name = 'LT_EfficientNetB0')\n",
    "efficientnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2113f6",
   "metadata": {},
   "source": [
    "### Modèle 3 : transfert learning de VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "vgg19 = EfficientNetB0(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(vgg19.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "vgg19 = Model(vgg19.input, output, name = 'TL_VGG19')\n",
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64160007",
   "metadata": {},
   "source": [
    "### Modèle 4 : transfert learning de Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "xception = Xception(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in xception.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(xception.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "xception = Model(xception.input, output, name = 'TL_Xception')\n",
    "xception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556c3ff",
   "metadata": {},
   "source": [
    "### Modèle 5 : transfert learning de ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(resnet.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "resnet = Model(resnet.input, output, name = 'TL_ResNet50V2')\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a1022",
   "metadata": {},
   "source": [
    "### Modèle 6 : transfert learning de InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d265566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "inceptionV3 = InceptionV3(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "for layer in inceptionV3.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "logits = Dense(2)(inceptionV3.layers[-1].output)\n",
    "output = Activation('softmax')(logits)\n",
    "inceptionV3 = Model(inceptionV3.input, output, name = 'TL_InceptionV3')\n",
    "inceptionV3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29282362",
   "metadata": {},
   "source": [
    "### Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42038d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "simple_model.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "efficientnet.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "vgg19.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "xception.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "resnet.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "inceptionV3.compile(optimizer=sgd, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f493495",
   "metadata": {},
   "source": [
    "### Entrainement des modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition de callbacks :\n",
    "earlystoper = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpointer_simple_model = ModelCheckpoint(filepath=f'{output_dir}/simple_model_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_efficientnet = ModelCheckpoint(filepath=f'{output_dir}/TL_EfficientNetB0_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_vgg19 = ModelCheckpoint(filepath=f'{output_dir}/TL_VGG19_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_xception = ModelCheckpoint(filepath=f'{output_dir}/TL_Xception_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_resnet = ModelCheckpoint(filepath=f'{output_dir}/TL_ResNet50V2_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "CSV_logger_simple_model = CSVLogger(filename = 'logger_simple_model.csv', separator=',', append = True)\n",
    "CSV_logger_efficientnet = CSVLogger(filename = 'logger_TL_EfficientNetB0.csv', separator=',', append = True)\n",
    "CSV_logger_VGG19 = CSVLogger(filename = 'logger_TL_VGG19.csv', separator=',', append = True)\n",
    "CSV_logger_xception = CSVLogger(filename = 'logger_TL_Xception.csv', separator=',', append = True)\n",
    "CSV_logger_resnet = CSVLogger(filename = 'logger_TL_ResNet50V2.csv', separator=',', append = True)\n",
    "\n",
    "callbacks_simple_model = [earlystoper, CSV_logger_simple_model, checkpointer_simple_model]\n",
    "callbacks_efficientnet = [earlystoper, CSV_logger_efficientnet, checkpointer_efficientnet]\n",
    "callbacks_vgg19 = [earlystoper, CSV_logger_VGG19, checkpointer_vgg19]\n",
    "callbacks_xception = [earlystoper, CSV_logger_xception, checkpointer_xception]\n",
    "callbacks_resnet = [earlystoper, CSV_logger_resnet, checkpointer_resnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a revoir....\n",
    "'''#définition d'une fonction de génération de callbacks\n",
    "def callbacks (model):\n",
    "    earlystoper = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    checkpointer = ModelCheckpoint(filepath=f'{output_dir}/TL_{model}_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "    callbacks = [earlystoper, checkpointer]\n",
    "    \n",
    "# définition d'une fonction permettant l'entrianement de modèles\n",
    "def trainer (model, optimizer, epochs, batch_size):\n",
    "    model.compile(optimizer=optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=len(val_df)//batch_size,\n",
    "                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                  callbacks=callbacks(model)\n",
    "                                 )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche de possibilité d'utilisation d'une GPU\n",
    "print('GPU Available : ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a revoir\n",
    "'''# entrainement des modèles\n",
    "optimizer = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "trainer(simple_model, optimizer, 5, 128)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement des différents modéles\n",
    "'''\n",
    "history_simple_model = simple_model.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                                  callbacks=callbacks_simple_model)\n",
    "\n",
    "history_efficientnet = efficientnet.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                                  callbacks=callbacks_efficientnet)\n",
    "\n",
    "history_vgg19 = vgg19.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                                  callbacks=callbacks_vgg19)\n",
    "\n",
    "history_xception = xception.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                                  callbacks=callbacks_xception)\n",
    "\n",
    "history_resnet = resnet.fit_generator(train_generator,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator,\n",
    "                                                  validation_steps=len(val_df)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df)//batch_size,\n",
    "                                                  callbacks=callbacks_resnet)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734519f8",
   "metadata": {},
   "source": [
    "### Entrainement des modèles sur datasets rééquilibrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa614790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition de callbacks :\n",
    "earlystoper = EarlyStopping(monitor='val_loss', patience=6)\n",
    "\n",
    "checkpointer_simple_model_X = ModelCheckpoint(filepath=f'{output_dir}/simple_model_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_efficientnet_X = ModelCheckpoint(filepath=f'{output_dir}/TL_EfficientNetB0_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_vgg19_X = ModelCheckpoint(filepath=f'{output_dir}/TL_VGG19_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_xception_X = ModelCheckpoint(filepath=f'{output_dir}/TL_Xception_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_resnet_X = ModelCheckpoint(filepath=f'{output_dir}/TL_ResNet50V2_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "checkpointer_InceptionV3 = ModelCheckpoint(filepath=f'{output_dir}/TL_InceptionV3_X_best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "CSV_logger_simple_model_X = CSVLogger(filename = 'logger_simple_model_X.csv', separator=',', append = True)\n",
    "CSV_logger_efficientnet_X = CSVLogger(filename = 'logger_TL_EfficientNetB0_X.csv', separator=',', append = True)\n",
    "CSV_logger_VGG19_X = CSVLogger(filename = 'logger_TL_VGG19_X.csv', separator=',', append = True)\n",
    "CSV_logger_xception_X = CSVLogger(filename = 'logger_TL_Xception_X.csv', separator=',', append = True)\n",
    "CSV_logger_resnet_X = CSVLogger(filename = 'logger_TL_ResNet50V2_X.csv', separator=',', append = True)\n",
    "CSV_logger_InceptionV3_X = CSVLogger(filename = 'logger_TL_InceptionV3_X.csv', separator=',', append = True)\n",
    "\n",
    "callbacks_simple_model_X = [earlystoper, CSV_logger_simple_model_X, checkpointer_simple_model_X]\n",
    "callbacks_efficientnet_X = [earlystoper, CSV_logger_efficientnet_X, checkpointer_efficientnet_X]\n",
    "callbacks_vgg19_X = [earlystoper, CSV_logger_VGG19_X, checkpointer_vgg19_X]\n",
    "callbacks_xception_X = [earlystoper, CSV_logger_xception_X, checkpointer_xception_X]\n",
    "callbacks_resnet_X = [earlystoper, CSV_logger_resnet_X, checkpointer_resnet_X]\n",
    "callbacks_InceptionV3_X = [earlystoper, CSV_logger_InceptionV3_X, checkpointer_InceptionV3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle simple sur le jeu de données rééquilibré\n",
    "history_simple_model_X = simple_model.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_simple_model_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle efficientnet sur le jeu de données rééquilibré\n",
    "history_efficientnet_X = efficientnet.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_efficientnet_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29410a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle vgg19 sur le jeu de données rééquilibré\n",
    "history_vgg19_X = vgg19.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_vgg19_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle xception sur le jeu de données rééquilibré\n",
    "history_xception_X = xception.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_xception_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle resnet sur le jeu de données rééquilibré\n",
    "history_resnet_X = resnet.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_resnet_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du modèle InceptionV3 sur le jeu de données rééquilibré\n",
    "history_InceptionV3_X = inceptionV3.fit_generator(train_generator_X,\n",
    "                                                  epochs=epochs,\n",
    "                                                  validation_data=val_generator_X,\n",
    "                                                  validation_steps=len(val_df_X)//batch_size,\n",
    "                                                  steps_per_epoch=len(train_df_X)//batch_size,\n",
    "                                                  callbacks=callbacks_InceptionV3_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63330d3f",
   "metadata": {},
   "source": [
    "### Affichage des GradCAMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c6427",
   "metadata": {},
   "source": [
    "#### simple_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_logit = Model(simple_model.input,simple_model.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_simple_model = GradCAM(model=simple_model_logit, layerName=\"maxpo2_block1_out\")\n",
    "retrained_guidedBP_simple_model = GuidedBackprop(model=simple_model, layerName=\"maxpo2_block1_out\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = simple_model.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == 1) & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == 0) & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(simple_model, retrained_gradCAM_simple_model,retrained_guidedBP_simple_model,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c6261",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_logit = Model(efficientnet.input,efficientnet.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_efficientnet = GradCAM(model=efficientnet_logit, layerName=\"top_conv\")\n",
    "retrained_guidedBP_efficientnet = GuidedBackprop(model=efficientnet, layerName=\"top_conv\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = efficientnet.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == 1) & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == 0) & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(efficientnet, retrained_gradCAM_efficientnet,retrained_guidedBP_efficientnet,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd63db",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b808811",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_logit = Model(vgg19.input,vgg19.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_vgg19 = GradCAM(model=vgg19_logit, layerName=\"top_conv\")\n",
    "retrained_guidedBP_vgg19 = GuidedBackprop(model=vgg19, layerName=\"top_conv\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = vgg19.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == '1') & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == '0') & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(vgg19, retrained_gradCAM_vgg19,retrained_guidedBP_vgg19,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878ba9c",
   "metadata": {},
   "source": [
    "#### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_logit = Model(xception.input,xception.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_xception = GradCAM(model=xception_logit, layerName=\"block14_sepconv2_act\")\n",
    "retrained_guidedBP_xception = GuidedBackprop(model=xception, layerName=\"block14_sepconv2_act\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = xception.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == 1) & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == 0) & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(xception, retrained_gradCAM_xception,retrained_guidedBP_xception,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf9d4f",
   "metadata": {},
   "source": [
    "#### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_logit = Model(resnet.input,resnet.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_resnet = GradCAM(model=resnet_logit, layerName=\"conv5_block3_out\")\n",
    "retrained_guidedBP_resnet = GuidedBackprop(model=resnet, layerName=\"conv5_block3_out\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = resnet.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == 1) & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == 0) & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(resnet, retrained_gradCAM_resnet,retrained_guidedBP_resnet,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75823da2",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea486af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionV3_logit = Model(inceptionV3.input,inceptionV3.layers[-2].output)\n",
    "\n",
    "retrained_gradCAM_inceptionV3 = GradCAM(model=inceptionV3_logit, layerName=\"conv2d_97\")\n",
    "retrained_guidedBP_inceptionV3 = GuidedBackprop(model=inceptionV3, layerName=\"conv2d_97\")\n",
    "\n",
    "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_gen.flow_from_dataframe(test_df, IMAGE_DIR, x_col=\"filename\",\n",
    "                                               target_size=(W,H), class_mode=None,\n",
    "                                                batch_size=1, shuffle=False)\n",
    "\n",
    "pred = inceptionV3.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "pred_indices = np.argmax(pred,axis=1)\n",
    "\n",
    "results = test_df.copy()\n",
    "results[\"pred\"] = pred_indices\n",
    "true_edible = list(results[(results.label == 1) & (results.pred ==1)].filename)\n",
    "true_inedible = list(results[(results.label == 0) & (results.pred ==0)].filename)\n",
    "wrong_class = [x for x in results.filename if x not in (true_inedible+true_edible)]\n",
    "\n",
    "# Affichage de GradCAM pour les champignons comestibles\n",
    "show_gradCAMs(inceptionV3, retrained_gradCAM_inceptionV3,retrained_guidedBP_inceptionV3,true_edible, n=5, decode={0:'0', 1:'1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda953a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
