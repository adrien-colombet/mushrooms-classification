{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7510cd2e",
   "metadata": {},
   "source": [
    "# Creation d'un premier CNN simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a67154",
   "metadata": {},
   "source": [
    "L'objectif de ce notebook est de créer un premier CNN simple qui servira de référence à la création d'un classifieur de comestibilité des champignons a partir d'une image.\n",
    "Les inputs de ce notebook sont :\n",
    "- le dataset d'images nettoyé et le fichier .csv correspondant au dataset d'images qui sera utilisé pour les parties train et test,\n",
    "- le dataset de validation avec le fichier .csv associé.\n",
    "\n",
    "Une visualisation durant l'apprentissage sera réalisée par le biais de GradCam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import  keras\n",
    "import tensorflow as tf # Utilisation de tensorflow v2.9.1\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création des liens vers les dossiers et fichiers source\n",
    "images_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\FFD_images_dataset'\n",
    "train_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\train_FFDataframe_full_undersampling.csv'\n",
    "test_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\test_FFDataframe_full_undersampling.csv'\n",
    "validation_dataset = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\val_FFDataframe_full.csv'\n",
    "\n",
    "# dossier ou sauver les résultats obtenus sur les modèles\n",
    "save_models_results = r'C:\\Users\\renamedadmin\\Documents\\Formation_Datascience\\Projet_Datascientest_Champignons\\Dossier_technique\\02_Pieces_constitutives\\Dataset\\Models_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de quelques fonctions utiles\n",
    "\n",
    "# affichage des metriques (accuracy, loss) d'entrainement d'un modèle\n",
    "def plot_scores(model, title):\n",
    "    '''\n",
    "    Arg :\n",
    "    model : model dont on souhaite afficher les metriques\n",
    "    Return:\n",
    "    plot des métriques Accuracy et loss sur les datasets train et test\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.rcParams['figure.figsize'] = [14,4]\n",
    "\n",
    "    # Créer la figure\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.gcf().subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0.3, hspace = 0.3)\n",
    "    # Créer les 4 graphiques\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # Tracer les données sur les graphiques\n",
    "    ax1.plot(model.history['accuracy'], label = \"train\")\n",
    "    ax1.plot(model.history['val_accuracy'], label = \"test\")\n",
    "    ax1.legend(loc = \"lower right\")\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')    \n",
    "\n",
    "    ax2.plot(model.history['loss'], label = \"train\")\n",
    "    ax2.plot(model.history['val_loss'], label = \"test\")\n",
    "    ax2.legend(loc = \"upper right\")\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')  \n",
    "    plt.title(title, loc = \"left\")\n",
    "    plt.show()\n",
    "    \n",
    "# affichage de la matrice de confusion du dataset de validation\n",
    "def show_confusion_matrix(model):\n",
    "    '''\n",
    "    Args :\n",
    "    model : modele à utiliser pour fair eles predictions\n",
    "   \n",
    "    Return :\n",
    "    plot de la matrice de confusion\n",
    "    '''\n",
    "    # réalisation des prédiction pour le modèle\n",
    "    model_pred=model.predict(val_generator, steps=val_steps, verbose=1)\n",
    "    y_pred = []\n",
    "    for element in model_pred:\n",
    "        pred = np.argmax(element)\n",
    "        y_pred.append(pred)\n",
    "    y_val = df_val.edible.to_list()\n",
    "    confusion_mtx = confusion_matrix(y_val, y_pred)\n",
    "    #\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mtx)\n",
    "    disp.plot(cmap='Blues', values_format='d', xticks_rotation='horizontal', colorbar = False)\n",
    "    plt.title(f'Confusion matrix for {model}')\n",
    "    plt.ylabel('True label', fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.xlabel('Predicted label', fontsize = 20)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des dataframes\n",
    "df_train = pd.read_csv(train_dataset)\n",
    "df_test = pd.read_csv(test_dataset)\n",
    "df_val = pd.read_csv(validation_dataset)\n",
    "\n",
    "# affichage de quelques infos sur ces dataframes + affichage d'une figure de répartition des catégories\n",
    "display(df_train.head(), df_test.info(), df_val.info())\n",
    "\n",
    "# génération des données du graph\n",
    "inedible = []\n",
    "edible = []\n",
    "\n",
    "dataframes = [df_train, df_test, df_val]\n",
    "for dataframe in dataframes:\n",
    "    count_inedible = dataframe['edible'].value_counts()[0]\n",
    "    inedible.append(count_inedible)\n",
    "    count_edible = dataframe['edible'].value_counts()[1]\n",
    "    edible.append(count_edible)   \n",
    "\n",
    "data = ['df_train', 'df_test', 'df_val']\n",
    "edibility = {'inedible': inedible, 'edible' : edible}\n",
    "\n",
    "colonnes = ['df_train', 'df_test', 'df_val']\n",
    "sex_counts = {\n",
    "    'inedible': inedible,\n",
    "    'edible': edible\n",
    "}\n",
    "\n",
    "width = 0.6\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "for i, j in edibility.items():\n",
    "    p = ax.bar(data, j, width, label=i, bottom=bottom)\n",
    "    bottom += j\n",
    "    ax.bar_label(p, label_type='center')\n",
    "ax.set_title('Number of images by category')\n",
    "ax.legend(title = 'categories')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c131a3b",
   "metadata": {},
   "source": [
    "## Mise en place d'un CNN d'architecture simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac83ed",
   "metadata": {},
   "source": [
    "### Création de générateurs de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de quelques paramètres\n",
    "batch_size = 64\n",
    "SEED = 3\n",
    "epochs = 15\n",
    "W, H = 224, 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset d'entrainement\n",
    "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_train[\"edible\"] = df_train[\"edible\"].apply(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(df_train, images_dataset,\n",
    "                                                    x_col=\"filename\",\n",
    "                                                    y_col=\"edible\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation du fonctionnement du data generator sur quelques images du dataset d'entrainement\n",
    "ex_df = df_train.sample(n=15).reset_index(drop=True)\n",
    "ex_gen = train_datagen.flow_from_dataframe(ex_df,images_dataset,\n",
    "                                           x_col=\"filename\",\n",
    "                                           y_col=\"edible\",\n",
    "                                           class_mode=\"categorical\")\n",
    "\n",
    "# affichage de quelquyes images issues du générateur\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(5,3,i+1)\n",
    "    for x, y in ex_gen:\n",
    "        im = x[0]\n",
    "        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset de test\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "df_test[\"edible\"] = df_test[\"edible\"].apply(str)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(df_test, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  y_col=\"edible\",\n",
    "                                                  class_mode=\"categorical\",\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319069b6",
   "metadata": {},
   "source": [
    "## Création du reseau simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = Sequential()\n",
    "# Convolution layer\n",
    "model_simple.add(Conv2D(filters=16,\n",
    "                 kernel_size=(3,3), \n",
    "                 padding='same',\n",
    "                 use_bias=False,\n",
    "                 input_shape=(224,224,3)))\n",
    "# Pooling layer\n",
    "model_simple.add(MaxPooling2D(pool_size=(4, 4),\n",
    "                       strides=(4, 4),\n",
    "                       padding='same'))\n",
    "# Second convolution layer\n",
    "model_simple.add(Conv2D(filters=32,\n",
    "                 kernel_size=(3,3), \n",
    "                 padding='same',\n",
    "                 use_bias=False))\n",
    "\n",
    "model_simple.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\n",
    "\n",
    "model_simple.add(GlobalAveragePooling2D())\n",
    "# Fully connected layers\n",
    "\n",
    "model_simple.add(Dense(2, activation='softmax'))\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b78399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation du modèle\n",
    "model_simple.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "# entrainement du modèle\n",
    "checkpointer_name = \"model_simple.hdf5\"\n",
    "CSV_logger_name = \"model_simple.csv\"\n",
    "checkpointer_model_simple= ModelCheckpoint(filepath=os.path.join(save_models_results, checkpointer_name),\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "CSV_logger_model_simple = CSVLogger(filename = CSV_logger_name,\n",
    "                                    separator=',',\n",
    "                                    append = True)\n",
    "callbacks_model_simple = [checkpointer_model_simple, CSV_logger_model_simple]\n",
    "start_time = time.time()\n",
    "history_model_simple = model.fit(train_generator,\n",
    "                                                    epochs=epochs,\n",
    "                                                    validation_data=test_generator,\n",
    "                                                    validation_steps=len(df_test)//batch_size,\n",
    "                                                    steps_per_epoch=len(df_train)//batch_size,\n",
    "                                                    callbacks=callbacks_model_simple)\n",
    "end_time = time.time()\n",
    "print(\"Durée de l'entrainement :\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(history_model_simple, \"scores du modèle CNN simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f7b4e",
   "metadata": {},
   "source": [
    "### Prédictions sur le dataset de validation et matrices de confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataGenerator pour le dataset de validation\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(df_val, images_dataset,\n",
    "                                                  x_col=\"filename\",\n",
    "                                                  class_mode=None,\n",
    "                                                  batch_size=1)\n",
    "val_steps = len(df_val)\n",
    "df_val[\"edible\"] = df_val[\"edible\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcda87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage du classification report du modele simple\n",
    "model_pred=model_simple.predict(val_generator, steps=val_steps, verbose=1)\n",
    "y_pred = []\n",
    "for element in model_pred:\n",
    "    pred = np.argmax(element)\n",
    "    y_pred.append(pred)\n",
    "y_val = df_val.edible.to_list()\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f414e1",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Ce premier modéle simple présente des résultats intéressants mais à tendance à classer de nombreuses images de champignons non-comestible comme étant comestibles ce qui est problématique... Dans de prochains notebook nous tenterons d'améliorer les résultats par utilisaitons de réseaux pré-entrainés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
